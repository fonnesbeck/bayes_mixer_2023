{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "akyAzKFsYHB5"
      },
      "source": [
        "# Bayesian Best Practices\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fonnesbeck/bayesian_workflow/blob/master/Bayesian_Best_Practices_with_PyMC.ipynb)\n",
        "\n",
        "Strengths of a Bayesian aproach that are relevant to data science:\n",
        "* Great flexibility to quickly and iteratively build statistical models\n",
        "* Offers principled way of dealing with uncertainty\n",
        "* Don't just want most likely outcome but distribution of all possible outcomes\n",
        "* Allows expert information to guide model by using informative priors\n",
        "\n",
        "In this tutorial you'll learn:\n",
        "* How to go from math to model\n",
        "* How to specify priors for your model\n",
        "* How to evaluate a model\n",
        "* How to iteratively improve a model\n",
        "\n",
        "We will follow a **workflow** for iterave improvement of our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQGVdH3SYHCC"
      },
      "source": [
        "## Data organization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZ208O-sYHCD",
        "outputId": "eabf684b-f556-4ca9-d814-439cd1663b48"
      },
      "outputs": [],
      "source": [
        "%env MKL_THREADING_LAYER=GNU\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "import arviz as az\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas_gbq as gbq\n",
        "import pymc as pm\n",
        "import pytensor.tensor as pt\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "print(f\"Running on PyMC v{pm.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoRiGqiPYHCD"
      },
      "source": [
        "Let's query for some data for estimating home run rate, including some potential covariates/group indicators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NK_i0IdM6euP"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "except ModuleNotFoundError:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvMbOFYrYHCH",
        "outputId": "777e4872-7d6c-40d0-890a-c7a0dda73aee"
      },
      "outputs": [],
      "source": [
        "query_string = \"\"\"\n",
        "select \n",
        "    pp.bam_id,\n",
        "    CONCAT(pp.last_name, \", \", pp.first_name) batter_name,\n",
        "    sp.team_id,\n",
        "    sp.season,\n",
        "    tm.name_abbrev,\n",
        "    pp.position,\n",
        "    pp.bats,\n",
        "    sp.hr,\n",
        "    sp.pa,\n",
        "    sp.avg_ev\n",
        "from\n",
        "    stats.view_stats_player_batting_byteam sp\n",
        "    inner join mesa.player pp using (bam_id)\n",
        "    join mesa.team tm on sp.team_id=tm.bam_id\n",
        "where sp.season>2020 \n",
        "    and tm.level='MLB' \n",
        "    and sp.game_type='R'\n",
        "    and sp.pa>0\n",
        "    and pp.position NOT IN ('P', 'LHS', 'LHR', 'RHS', 'RHR')\n",
        "order by bam_id\n",
        "\"\"\"\n",
        "\n",
        "hr_data = gbq.read_gbq(query_string, project_id=\"phil-new\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "OFjBLaMVYHCI",
        "outputId": "e3b0efee-1f99-4547-c920-9c7c8420ffc8"
      },
      "outputs": [],
      "source": [
        "hr_data['label'] = hr_data.batter_name + ' (' + hr_data.name_abbrev + ') ' + hr_data.season.astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9z0zOJlIRTlg"
      },
      "outputs": [],
      "source": [
        "fitting_subset = hr_data[hr_data.season<2023].dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fitting_subset.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k_1VV_25YHCT"
      },
      "source": [
        "Distribution of home run rates:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "vAIN3LmVYHCT",
        "outputId": "abc7ce02-ad6c-4f31-8c24-801fbd53bb58"
      },
      "outputs": [],
      "source": [
        "sns.histplot(x='hr_rate', \n",
        "             data=fitting_subset.assign(hr_rate=(fitting_subset.hr/fitting_subset.pa).astype(np.float32)),\n",
        "             bins=25\n",
        ");"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A natural likelihood function for this data is the binomial distribution. The binomial distribution is a discrete probability distribution that models the number of successes in a sequence of $n$ independent success/failure events, each of which yields success with probability $p$. Such a success/failure experiment is also called a **Bernoulli trial**; when $n = 1$, the binomial distribution is a Bernoulli distribution. \n",
        "\n",
        "$$y_i \\sim \\text{Binomial}(n_i, p_i)$$\n",
        "\n",
        "## Choosing a prior\n",
        "\n",
        "\n",
        "\n",
        "$$ p_i = \\text{Uniform}(0, 1) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpIahH19UZs_"
      },
      "outputs": [],
      "source": [
        "pa, hr = fitting_subset[['pa', 'hr']].astype(int).values.T\n",
        "coords = {'batter':fitting_subset.label.values}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "jU_JaPNMYHCc",
        "outputId": "cae5864c-716a-496e-b3d2-17f8dd5eccb5"
      },
      "outputs": [],
      "source": [
        "with pm.Model(coords=coords) as uninformative_prior_model:\n",
        "    \n",
        "    p = pm.Uniform('p', 0, 1, dims='batter')\n",
        "    \n",
        "    y = pm.Binomial('y', n=pa, p=p, observed=hr, dims='batter')\n",
        "    \n",
        "pm.model_to_graphviz(uninformative_prior_model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prior Predictive Sampling\n",
        "\n",
        "Before attempting to fit this model, let's take a look at this (admittedly simple) model and see what it implications are before any data are brought to bear on the problem. To do this, we can generate data from the **prior predictive distribution**. This is a good idea to do before fitting any model, as it allows us to check that our model is reasonable and that our inference algorithms are working.\n",
        "\n",
        "$$\n",
        "p\\left(y\\right)=\\int_{\\Theta} p\\left(y, \\theta\\right) \\mathrm{d} \\theta=\\int_{\\Theta} p\\left(y \\mid \\theta\\right) p(\\theta) \\mathrm{d} \\theta\n",
        "$$\n",
        "\n",
        "This distribution is predictive in the sense that its predicting the behavior of new data that is not currently part of the model.\n",
        "\n",
        "Notice we are not conditioning on any data here! We are seeing what the model predicts in the absence of data.\n",
        "\n",
        "Sampling from the prior predictive distribution is easy: we draw a value for $p$ from the prior distribution, and then sample a value for $y$ from the binomial distribution conditional on the drawn value of $p$. We can repeat this process as many times as we need to generate a sample from the prior predictive distribution.\n",
        "\n",
        "In PyMC, this is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOLcE1MmU9co"
      },
      "outputs": [],
      "source": [
        "with uninformative_prior_model:\n",
        "    uninformed_prior_trace = pm.sample_prior_predictive(1000)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The prior predictive sampler (like all samplers in PyMC), yeilds a custom data structure upon completion, an `InferenceData` object from the ArviZ library. This object contains the samples from the prior predictive distribution, as well as some metadata about the model and the sampling process. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "uninformed_prior_trace"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at the distribution of home run rates, which unsurprisingly are uniformly distributed between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.histplot(uninformed_prior_trace.prior['p'].sel(chain=[0], draw=[0]).squeeze(), bins=25);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The posterior predictive samples are simulated home runs for each player in the dataset, given the number of plate appearances passed to the binomial sampling distribution. Right away, it is apparent that the model behaves poorly out of the box, but remember it has not yet been exposed to any of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "4SlFhAALVJZg",
        "outputId": "f4aa1be9-f2a0-4ced-c6b4-e07d47168189"
      },
      "outputs": [],
      "source": [
        "az.plot_ppc(uninformed_prior_trace, group='prior', kind='cumulative');"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Fitting with MCMC\n",
        "\n",
        "Let's go ahead and estimate our model using Markov chaine Monte Carlo (MCMC); specifically, PyMC will use the NUTS algorithm since all of our parameters are continuous.\n",
        "\n",
        "How many samples do we need? How long should we tune?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "pbjM_eseYHCd",
        "outputId": "0c3e60fb-2a5d-4427-ffd9-fc06f147582a"
      },
      "outputs": [],
      "source": [
        "with uninformative_prior_model:\n",
        "    trace = pm.sample(draws=100, tune=100, cores=4, chains=4, random_seed=RANDOM_SEED)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pepFaNEKYHCe"
      },
      "source": [
        "Notice that we have been issued a couple of warnings, one regarding the size of **rhat** statistics and another regarding the effective sample size. These are both **convergence diagnostics** that are used to assess whether the MCMC algorithm has converged to the target distribution.\n",
        "\n",
        "### $\\hat{R}$ Diagnostic\n",
        "\n",
        "The R-hat diagnostic uses multiple chains to\n",
        "check for lack of convergence, and is based on the notion that if\n",
        "multiple chains have converged, by definition they should appear very\n",
        "similar to one another; if not, one or more of the chains has failed to\n",
        "converge.\n",
        "\n",
        "It calculates both the between-chain\n",
        "varaince (B) and within-chain varaince (W), and assesses whether they\n",
        "are different enough to worry about convergence. Assuming $m$ chains,\n",
        "each of length $n$, quantities are calculated by:\n",
        "\n",
        "$$\\begin{align}B &= \\frac{n}{m-1} \\sum_{j=1}^m (\\bar{\\theta}_{.j} - \\bar{\\theta}_{..})^2 \\\\\n",
        "W &= \\frac{1}{m} \\sum_{j=1}^m \\left[ \\frac{1}{n-1} \\sum_{i=1}^n (\\theta_{ij} - \\bar{\\theta}_{.j})^2 \\right]\n",
        "\\end{align}$$\n",
        "\n",
        "for each scalar estimand $\\theta$. Using these values, an estimate of\n",
        "the marginal posterior variance of $\\theta$ can be calculated:\n",
        "\n",
        "$$\\hat{\\text{Var}}(\\theta | y) = \\frac{n-1}{n} W + \\frac{1}{n} B$$\n",
        "\n",
        "Assuming $\\theta$ was initialized to arbitrary starting points in each\n",
        "chain, this quantity will overestimate the true marginal posterior\n",
        "variance. At the same time, $W$ will tend to underestimate the\n",
        "within-chain variance early in the sampling run. However, in the limit\n",
        "as $n \\rightarrow \n",
        "\\infty$, both quantities will converge to the true variance of $\\theta$.\n",
        "In light of this, the Gelman-Rubin statistic monitors convergence using\n",
        "the ratio:\n",
        "\n",
        "$$\\hat{R} = \\sqrt{\\frac{\\hat{\\text{Var}}(\\theta | y)}{W}}$$\n",
        "\n",
        "This is called the potential scale reduction, since it is an estimate of\n",
        "the potential reduction in the scale of $\\theta$ as the number of\n",
        "simulations tends to infinity. In practice, we look for values of\n",
        "$\\hat{R}$ close to one (say, less than 1.1) to be confident that a\n",
        "particular estimand has converged. \n",
        "\n",
        "In ArviZ, the function\n",
        "`rhat` will calculate $\\hat{R}$ for each stochastic node in\n",
        "the passed model. Specifically, it calulates split-$\\hat{R}$, which compares the first half to the second half of each chain, to allow for within-chain detection of lack of convergence.\n",
        "\n",
        "### Effective sample size\n",
        "\n",
        "The effective sample size is estimated using the partial sum:\n",
        "\n",
        "$$\\hat{n}_{eff} = \\frac{mn}{1 + 2\\sum_{i=1}^T \\hat{\\rho}_i}$$\n",
        "\n",
        "where $T$ is the first odd integer such that $\\hat{\\rho}_{T+1} + \\hat{\\rho}_{T+2}$ is negative. The $\\hat{\\rho}_i$ are the lag-$i$ autocorrelation estimates, which are calculated using the standard autocorrelation function:\n",
        "\n",
        "$$\\hat{\\rho}_i = 1 - \\frac{V_i}{2\\hat{\\text{Var}}(\\theta | y)}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "az.summary(trace).sort_values('r_hat', ascending=False).head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bulk-ESS is useful as a diagnostic for the sampling efficiency in the bulk of the posterior. It is defined as the effective sample size for rank normalized values using split chains. \n",
        "\n",
        "Tail-ESS is useful as a diagnostic for the sampling efficiency in the tails of the posterior. It is defined as the minimum of the effective sample sizes for 5% and 95% quantiles.\n",
        "\n",
        "---\n",
        "\n",
        "Clearly, 100 samples after 100 tuning samples is not sufficient to estimate the posterior distribution. Let's bump things up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with uninformative_prior_model:\n",
        "    trace = pm.sample(draws=1000, tune=1000, cores=4, chains=4, random_seed=RANDOM_SEED)\n",
        "\n",
        "# Add samples to the InfrernceData object\n",
        "uninformed_prior_trace.extend(trace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "az.plot_ess(uninformed_prior_trace, var_names=[\"p\"], coords={'batter': ['Schwarber, Kyle (PHI) 2022']}, kind=\"evolution\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Bc12mrX8YHCf",
        "outputId": "b1236efa-0940-47c2-a00f-1154be6b2031",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "az.plot_forest(\n",
        "    uninformed_prior_trace, var_names=\"p\", figsize=(5, 180), r_hat=True, combined=True, textsize=8\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDnOWE_hYHCf"
      },
      "source": [
        "Sampling was good for all players, but you can see that some are far more uncertain than others, and this uncertainty is related to the number of plate appearances, as we would expect. \n",
        "\n",
        "To identify batters with high home run rates, we can plot the ordered mean estimates, as well as their 94% HPD:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JexSK0wbYHCf"
      },
      "outputs": [],
      "source": [
        "uninformed_means = uninformed_prior_trace.posterior.mean(dim=(\"chain\", \"draw\"))\n",
        "uninformed_hdi = az.hdi(uninformed_prior_trace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "id": "Yni3nMe3YHCf",
        "outputId": "bdaa7e22-bcf5-4161-fd61-2d699e3b6e72"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "uninformed_means_iter = uninformed_means.sortby(\"p\")\n",
        "uninformed_hdi_iter = uninformed_hdi.sortby(uninformed_means_iter.p)\n",
        "ax.vlines(\n",
        "    np.arange(fitting_subset.shape[0]),\n",
        "    uninformed_hdi_iter.p.sel(hdi=\"lower\"),\n",
        "    uninformed_hdi_iter.p.sel(hdi=\"higher\"),\n",
        "    color=\"orange\",\n",
        "    alpha=0.6,\n",
        "    linewidth=0.6\n",
        ")\n",
        "uninformed_means_iter.plot.scatter(x=\"batter\", y=\"p\", ax=ax, alpha=0.8)\n",
        "\n",
        "ax.set_xticklabels([])\n",
        "ax.set_xticks([])\n",
        "sns.despine(offset=10, trim=True, bottom=True);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVP6QtQxYHCq"
      },
      "source": [
        "Of concern here is the tail of high home run rates; there are a lot of them, and they are all highly uncertain. If we plot the interval width against the number of plate appearances, we can see that sampling error is indeed a big factor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "M2NHDO5_YHCq",
        "outputId": "2a64d964-5391-4ed1-e10a-09e39eba0d95"
      },
      "outputs": [],
      "source": [
        "uncertainty = uninformed_hdi.p.sel(hdi=\"higher\") - uninformed_hdi.p.sel(hdi=\"lower\")\n",
        "\n",
        "plt.figure(figsize=(11, 6))\n",
        "plt.plot(pa, uncertainty, \"o\", alpha=0.4)\n",
        "plt.xlabel(\"Plate appearances\")\n",
        "plt.ylabel(\"Uncertainty of estimate\");"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pAYaYhznYHCr"
      },
      "source": [
        "This is a classic issue with no-pooling models: when you estimate individuals as being independent from each other, how do you interpret the small-sample-size estimates?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4olvqwqkYHCr"
      },
      "source": [
        "Let's look closer at a handful of random examples from the dataset. Below are the posterior distributions of rates for 20 batters (blue intervals), along with the empirical rates (red dots). \n",
        "\n",
        "This illustrates how the Bayesian posterior is a (weighted) compromise between the prior (uniform over the [0,1] interval) and the data (observed home runs and plate appearances). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "ZChFglbIlNcv",
        "outputId": "47acf43f-9b2f-4f6a-967e-2758a54544f5"
      },
      "outputs": [],
      "source": [
        "SAMPLE_BATTERS = fitting_subset.sample(n=20, random_state=42).label\n",
        "obs_rates = hr_data.set_index('label').loc[SAMPLE_BATTERS].apply(lambda x: x.hr/x.pa, axis=1)\n",
        "\n",
        "(ax,) = az.plot_forest(uninformed_prior_trace, coords={'batter': list(SAMPLE_BATTERS)}, combined=True)\n",
        "ax.plot(obs_rates[::-1], ax.get_yticks(), 'ro');\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RtZ1xEzGO8bA"
      },
      "source": [
        "Note that all of the empirical rates lie within the posterior interval of the estaimtes, however wide or narrow they end up being. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Posterior Predictive Sampling\n",
        "\n",
        "We can also use the posterior distribution to generate predictions for new data. This is is done via the **posterior predictive distribution**, which is a distribution over new data, conditional on the observed data. \n",
        "\n",
        "$$\n",
        "p\\left(y^{\\mathrm{new}} \\mid y^{\\mathrm{obs}}\\right)=\\int p\\left(y^{\\mathrm{new}} \\mid \\theta\\right) p(\\theta \\mid y^{\\mathrm{obs}}) \\mathrm{d} \\theta\n",
        "$$\n",
        "\n",
        "This will give us an idea about absolute model performance after it has been updated with data. It essentially sinulates new datasets, which can then be compared to the dataset that was actually observed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with uninformative_prior_model:\n",
        "    pm.sample_posterior_predictive(uninformed_prior_trace, extend_inferencedata=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use `plot_ppc` again, but this time with the \"posterior\" group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "1fHN_Pq8Og-Q",
        "outputId": "08bb030f-e189-4b1c-b265-9cefbefc6811"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "az.plot_ppc(uninformed_prior_trace, group='posterior', kind='cumulative', ax=axes[0]);\n",
        "az.plot_ppc(uninformed_prior_trace, group='posterior', ax=axes[1]);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model fits the dataset reasonably well, though the estiamtes for low-sample size home run rates are not great."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Informative Priors\n",
        "\n",
        "An advantage of using Bayesian models is the ability to incorporate prior information into the model. This is especially useful when data are sparse, as with batters having few plate appearances. In order to implement an informative prior here, we need to move away from the uniform prior on $p$ and choose a different distribution. An obvious choice is the **beta distribution**, which is a continuous distribution on the interval [0,1] that is parameterized by two positive shape parameters, $\\alpha$ and $\\beta$. The beta distribution is a conjugate prior for the binomial distribution, which means that the posterior distribution will be a beta distribution as well.\n",
        "\n",
        "$$ p(x \\mid \\alpha, \\beta) =\n",
        "           \\frac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{B(\\alpha, \\beta)} $$\n",
        "\n",
        "The two shape parameters make the beta distribtion pretty flexible:\n",
        "\n",
        "![](https://www.pymc.io/projects/docs/en/stable/_downloads/fc694eda09419e702a96ca1c2031e2a6/pymc-Beta-1.png)\n",
        "\n",
        "The trick is to choose the right shape parameters to encode our prior information, which may not be intuitive. We can try a few different values and see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(12, 8), sharex=True, sharey=True)\n",
        "\n",
        "for params, ax in zip([(1, 15), (1, 20), (1.5, 30), (2, 40)], axes.flat):\n",
        "    sns.histplot(pm.Beta.dist(*params, shape=1000).eval(), ax=ax, stat='probability');\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In PyMC, it is helpful to use the `find_constrained_prior` function to choose appropriate hyperparameters for a given distribution. For exmaple, we might be interested in a beta distribtion with 95% of its probability mass between 0.01 and 0.11. This information is used to optimize the shape parameters of the beta distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pm.find_constrained_prior(\n",
        "    pm.Beta,\n",
        "    lower=0.01,\n",
        "    upper=0.11,\n",
        "    init_guess={\"alpha\": 2, \"beta\": 40},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with pm.Model(coords={'hitter':fitting_subset.label.values}) as informative_prior_model:\n",
        "    \n",
        "    beta_params = pm.find_constrained_prior(\n",
        "        pm.Beta,\n",
        "        lower=0.005,\n",
        "        upper=0.09,\n",
        "        init_guess={\"alpha\": 2, \"beta\": 40},\n",
        "    )\n",
        "\n",
        "    p = pm.Beta('p', **beta_params, dims='hitter')\n",
        "    \n",
        "    y = pm.Binomial('y', n=pa, p=p, observed=hr, dims='hitter')\n",
        "    \n",
        "pm.model_to_graphviz(informative_prior_model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ### Empirical Bayes\n",
        "> One way to obtain informative priors is to derive a distribution from **population data**. This is not “purely” Bayesian, since in a sense we are using the data to determine the prior specification. Furthermore, the estimation of ψ must be done with non-Bayesian techniques. The resulting prior is thought to represent the population of all possible values of the unknown parameter, and is called an **empirical Bayes** prior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with informative_prior_model:\n",
        "    informed_prior_trace = pm.sample_prior_predictive(1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "az.plot_ppc(informed_prior_trace, group='prior', kind='cumulative');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with informative_prior_model:\n",
        "    informed_prior_trace.extend(pm.sample(cores=4, chains=4, random_seed=RANDOM_SEED))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "informed_means = informed_prior_trace.posterior.mean(dim=(\"chain\", \"draw\"))\n",
        "informed_hdi = az.hdi(informed_prior_trace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "informed_means_iter = informed_means.sortby(\"p\")\n",
        "informed_hdi_iter = informed_hdi.sortby(informed_means_iter.p)\n",
        "ax.vlines(\n",
        "    np.arange(fitting_subset.shape[0]),\n",
        "    informed_hdi_iter.p.sel(hdi=\"lower\"),\n",
        "    informed_hdi_iter.p.sel(hdi=\"higher\"),\n",
        "    color=\"orange\",\n",
        "    alpha=0.3,\n",
        "    linewidth=0.6\n",
        ")\n",
        "informed_means_iter.plot.scatter(x=\"hitter\", y=\"p\", ax=ax)\n",
        "\n",
        "ax.set_xticklabels([])\n",
        "ax.set_xticks([])\n",
        "sns.despine(offset=10, trim=True, bottom=True);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One thing that you may have noticed is that we treat every player-season as independent. However, repeated measures of individual players are **correlated**. We can improve our model to account for this."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HanSuJFSYHCs"
      },
      "source": [
        "## Multilevel/Hierarchical models\n",
        "\n",
        "When we analyze data unpooled, we imply that they are sampled independently, from separate models. This approach claims that differences between players are too severe to combine them -- we assume that batters are underelated:\n",
        "\n",
        "![uninformative_prior_model.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAosAAADmCAIAAAAyfVjtAAAYKWlDQ1BJQ0MgUHJvZmlsZQAAWAmteXk8Vd/X/z7nToZrnudrnud5nqeQeYxyXfNw4xoTDZQKqSTJECFTISpTKBqkRKKiVCQhCimV8TlX9fl8v6/n9/z3u6/XPed9137vtddea5999loXAN4bxMjIcJgZgAhyDMXJ0oTg4elFwL0BdAADGIEqkCaSoiONHRxswf/5WXkBIGrjkDxV1/9J+383sPgHRJMAgByQZj//aFIEgm8AgGohRVJiAMBQ9YnFx0RS8WkEs1MQAxFcTsVBv3ELFfv9xn3bHBcnU4QzDgANnkikBAHAMI/ICXGkIEQPIx4ALCvZP4SMdCMg2IAUTPQHgNcX4chFROyl4kwES/n9h56g/8BEot8/OonEoH/w77kgPZGBzUKiI8OJ+7Z//P+8RITHIv7a/ggiV3x0mLMNcudE/JZAIpo7I5gbwVnBAda2f+QVkTEmTn/k7SEx1i4IZkc4w8GxVq5/8ExsmKsxgvkR+XrYXhsqH/ETzE3222mPYFYEi5GiTRHfU8eCNRKDXdz/cGz9A8zMEYysItiDstfpLz84Os75rzwxMdh0519+KHEHNd6MCD+dSEHQtj1wXkC4JXVcEUR+NTLGgWondax+cvjOP3OBPwRSLKgcqnw1IHp7vlTbgmOCXawQOWIzijmG4kLlIHNE8QeGWFgjGLENpRRMsforN4oM317TSF+UCyXWieoHMQQHBpBdqT6kytP9iWZU3yI+QRUCC0AEFBAA/AAZzAICsAWmwOzPlYDIyYiMBPaCcORLITD9bcF8xAxi3mOeY8Yxo39lSM8/PBAC/BH8W9d/9EfkziARfEa0BoDov6OhedEGaF20LXI1Qr4qaC209t+2/vnm+b/4j61BSF/5P7pN/lgfh2jc+MvbE5JC+Yv/9PH7p8f/tskCfEA8EPSXoXRFaVZp/W//f2eMNceaYa2wFlhp1HHUdVQPqgvVi2pHNQMC6jaqBdWH6qDiP3b9HYWISKheoXo4GtggXgwAsdu/yH/H+y8vxf7D+KOBUYZRHTghvcggDGkL+WcEt22rQ/6XlliE4YeMGIpwbf6Jxx+70BKId9XRJmh9xM+Ij9GcaF4gj1ZDPG6MNkRioI5I/43if89GHgRueztuey5h4CMyj4iYgIQYZC0B072R+yghQcExBGNktwyQI1iTSQpyBBUlZVVA3XupHACWnbb3VIhz4F+ZnzIAWhvI1tL3ryxiHYC6IwAI3v9XJlGNbAEHALhSSIqlxP3Wh6beMMiuzoQ8FTxAEIgCKcQjKkAD6AIjYA52AHvgAjzBbmQNB4MIxOJ4kAQOgzSQAU6DcyAfFIMyUAVqQSNoBu2gCzwAj8FT8By8BuNgCsyBRbAC1iAIwkEMEBvEAwlB4pAspAJpQQaQOWQLOUGekC8UBJGhWCgJSoUyoGwoH7oEVUMNUCvUBfVCg9AoNAHNQl+hVRgF42F2WACWgBVhLdgYtoFdYB84CI6CE+EjcBacB5fCV+EmuAt+DD+Hx+E5+DsKoOhRnChhlDxKC2WKskd5oQJRFNQBVDoqF1WKqkO1IWtxCDWOmkf9QmPRbGgCWh6JpBXaFU1CR6EPoDPR+egqdBP6HnoIPYFeRG9iGDD8GFmMDsYa44EJwsRj0jC5mArMTcx95HmewqxgsVhOrCRWE1ntnthQ7H5sJrYIW4+9gx3ETmK/43A4HpwsTh9njyPiYnBpuAu4q7jbuGe4KdxPGnoaIRoVGgsaLxoyTQpNLk0NTSfNM5ppmjVaZlpxWh1ae1p/2n20p2jLadtoB2inaNfoWOgk6fTpXOhC6Q7T5dHV0d2nG6NbpqenF6HXpnekD6E/RJ9Hf43+If0E/S88K14Gb4r3xsfis/CV+Dv4UfwyAwODBIMRgxdDDEMWQzXDXYa3DD8Z2RgVGK0Z/RkPMhYwNjE+Y/zCRMskzmTMtJspkSmX6TrTANM8My2zBLMpM5H5AHMBcyvzS+bvLGwsyiz2LBEsmSw1LL0sM6w4VglWc1Z/1iOsZax3WSfZUGyibKZsJLZUtnK2+2xT7Fh2SXZr9lD2DPZa9n72RQ5WDjUON44EjgKODo5xThSnBKc1ZzjnKc5Gzhecq1wCXMZcAVwnuOq4nnH94ObjNuIO4E7nrud+zr3KQ+Ax5wnjOcPTzPOGF80rw+vIG897kfc+7zwfO58uH4kvna+R7xU/zC/D78S/n7+Mv4//u4CggKVApMAFgbsC84KcgkaCoYI5gp2Cs0JsQgZCIUI5QreFPhE4CMaEcEIe4R5hUZhf2Eo4VviScL/wmoikiKtIiki9yBtROlEt0UDRHNFu0UUxITE7sSSxK2KvxGnFtcSDxc+L94j/kJCUcJc4JtEsMSPJLWktmSh5RXJMikHKUCpKqlRqWBorrSUdJl0k/VQGllGXCZYpkBmQhWU1ZENki2QH5TBy2nJkuVK5l/J4eWP5OPkr8hMKnAq2CikKzQpfFMUUvRTPKPYobiqpK4UrlSu9VmZV3qGcotym/FVFRoWkUqAyrMqgaqF6ULVFdUlNVi1A7aLaiDqbup36MfVu9Q0NTQ2KRp3GrKaYpq9moeZLLXYtB61MrYfaGG0T7YPa7dq/dDR0YnQadRZ05XXDdGt0Z/Qk9QL0yvUm9UX0ifqX9McNCAa+BiUG44bChkTDUsP3RqJG/kYVRtPG0sahxleNv5gomVBMbpr8MNUxTTa9Y4YyszRLN+s3ZzV3Nc83f2shYhFkccVi0VLdcr/lHSuMlY3VGauX1gLWJOtq68UdmjuSd9yzwds42+TbvLeVsaXYttnBdjvsztqN7RTfSd7ZbA/sre3P2r9xkHSIcrjliHV0cCxw/Oik7JTk1OPM5rzHucZ5xcXE5ZTLa1cp11jXbjcmN2+3arcf7mbu2e7jHooeyR6PPXk9QzxbvHBebl4VXt93me86t2vKW907zfuFj6RPgk/vbt7d4bs79jDtIe657ovxdfet8V0n2hNLid/9rP0K/RZJpqTzpDl/I/8c/9kA/YDsgOlA/cDswJkg/aCzQbPBhsG5wfMhpiH5IUuhVqHFoT/C7MMqw7bC3cPrI2gifCNayazkMPK9vYJ7E/YORspGpkWOR+lEnYtapNhQKqKhaJ/olhh25JDbFysVezR2Is4griDuZ7xb/PUElgRyQt8+mX0n9k0nWiRe3o/eT9rfnSScdDhpItk4+dIB6IDfge6DogePHJw6ZHmo6jDd4bDDT1KUUrJTvqW6p7YdEThy6MjkUcujV9IY0yhpL4/pHis+jj4ecrz/hOqJCyc20/3TH2UoZeRmrGeSMh+dVD6Zd3IrKzCr/5TGqYunsafJp1+cMTxTlc2SnZg9edbubFMOISc959u5Ped6c9Vyi8/TnY89P55nm9dyQezC6Qvr+cH5zwtMCuoL+QtPFP4o8i96dtHoYl2xQHFG8WpJSMnIJctLTaUSpbll2LK4so/lbuU9l7UuV1fwVmRUbFSSK8ernKruVWtWV9fw15y6Al+JvTJ71fvq01qz2pY6+bpL9Zz1GdfAtdhrnxp8G1402jR2X9e6XndD/EbhTbab6U1Q076mxebg5vEWz5bB1h2t3W26bTdvKdyqbBduL+jg6DjVSdd5pHPrduLt73ci78x3BXVNdu/pfn3X4+7wPcd7/fdt7j98YPHgbo9xz+2H+g/be3V6Wx9pPWp+rPG4qU+97+YT9Sc3+zX6mwY0B1qeaj9tG9Qb7Hxm+KxryGzowbD18OPnO58PvnB9MfLS++X4iP/IzGj46NKruFdrrw+NYcbS3zC/yX3L/7b0nfS7+nGN8Y4Js4m+987vX0+SJuc+RH9YnzrykeFj7rTQdPWMykz7rMXs00+7Pk3NRc6tzad9Zvlc+EXqy40Fo4W+RY/FqSXK0tbXzGWe5cpvat+6vzt8f7sSsbL2I/0nz8+qX1q/elbdV6fX4tdx63kb0httmzabY1sRW1uRRApx+yyAQq5wYCAAXyuRvMgTALanANAx/s6NthnIcRdCOAh2gxSgOfgISgU1gy7BEJF33SKuhSaTNojOkJ4dz8zAysjMxM7MzyLMKsumw27H4ceZwHWWu46nj3eG7zv/hiCNEDdBXFhJxEDUXsxXPFriuGSRVKP0E1mMHFG+U5GgtF95WFVGLV69WxOrZaodo1Om26e3YEBnyGLEYcxtwmPKa8Zlzm7BZEljuWX1zXpuxzubIdsHds07q+zzHU46pjolOse4RLlGu8W7H/A47pnjVbqr1rvOp2Z3xZ4y30vEYr8S0iX/8oDqwPqg1uC7IQOhb8OWIrBk3r2ykTpRVhTX6N0xAbHhcVHxcQn79iUnHtpflNSWPHTg08HNw4wpfKkSRxSOqqfpHDM4bnLCMt0xg5JZeLI36+dp6TPe2SfPdubM53Ke183zvpCQf7qgrLCp6NHFN8VLl9ClnGVS5dqXrSs8KoOqYqtTarKuFFy9XNtQd7t+4Np4w/J17A2+m0pN5s2eLRGth9qyb11qP9kR0+l5W/+OUBfoet99927FvYz7lAc+PTse6vQK9s49Kn/s3oftq3/i8uRnf/6A4cDU07OD5oPrz24NpQzbPxd6/uVFx8v0EddRodGZV7Wvo8ZUx769aXwb+U7l3cZ4/0Tx+8hJow8sH95PNXw8PO0wIzTzefbWpxNzLvM886Ofz3/xWhBd+LU4vbS0LP3t1IrsT6ZVx/X5ra3t+ItC12BbeBGVi7ZAf8dUYv1wYrgZmjraGDpjenE8C36VYY7xNdNT5vssnazNbC3s7RwdnF1cvdyDPD28HXxN/A0CVwSrhCoIF4XPipwWLRPrEf8iySllJB0uky/7UG5FQULRWemAco3KoOqKOreGlqa9FlE7VIek66lnp29soGYobsRuDBsvmLw2fWB2zTzf4ohluJWTtcYOPhtgM23bb3dzZ7H9CYc4R5KTk7OJi4qrmBunO437msdnz3deg7vueTcjq6F4T45vOvGQXxyJ7E8OIAdGBIUHh4WEhgaHBYQTI3aTvfa6RzpH2VPsoj1jApEjc3J8WkLWvpzEC/sLk4qSiw+UHCw5VHK4LKUptf/ITBrmmAiyHkjpqRllmV0n32b9OM14RjRb86xtjt+5hNxT58vz2i4M5E8W/CiivchbLFuic8my1K0suPzA5ZyK2sqHVR9qwBWBq7q17nWU+sxrlxvaG59ef3Nj9ub3ZqiFoZWnTfyWUrt2h0nnjtsOd1y7vLp33XW5Z31f74F8D99D3MPF3hePWh8X9CU98e7XHeAd+Pn0xeCNZ2eHYoY9nhu9kH8pNMI9yvVK8LXMmN4b57d+77zGLSfU3gtPMk5ufvgyNfHx5fTAzOPZ3k+9c0/mhz5/WoAWuZbkvpot+35L/d6wMvdT71fJGvd6zabtdvxlwT3IBhpBVsADlDXqOdoX/Q2ThhXGNuOcccs0lbQ+dGx0T+mz8E4MBIZFxkdMV5lzWA6zxrCFsftw6HNycy4jK6GEJ47Xhk+Y7wf/E4FywWQhd4KqMLPwtEir6HExF3EB8XGJUkmSlLjUR+lqmb2yWnJouUH5fAWiopTivFKDcpyKjsqmapfaEXVLDRqNR5oZWnbatNoPdFJ1jXTX9Vr0YwyUDD4ZXjbyNeY1HjbJMrU23TS7bh5qIWQxZHnMStfqs3XxDkcbyKbR1s+Oxe7+zmR7HfsfDjccKcj5YcG5zoXsKu/62a3WPcJDzmPGs9TLcxf7rkHv0z7Ou3l2v99T57ufaOPH7/eF1OWfHeAdKBo4F3Q9OCnEPJQ5dCysJjwxwo4sQv6191lkZVQkRZWyEt0akxxrGoePG4mvTEja556ouZ8vCZX0JXn0wN2DtYfOHz6cEpbqdsT4qEKawDGGY5vHl058TH+Z0Zl56WRqVsApq9PyZ9jObGR/OjuW8+zck9zH5wfyhi+M5I8VvCv8UDRzcaF45RIopS/jKZe+rFfhWBlcdag6r+balZ6r47W/6tmuyTdYNwZdT7tRcfNh03wLS6thW9qtoQ7BzsDbtXeWu3XvHr03+ECsJ+nh60cmjxueyPQXP0UPEp91Das8b3tJGSW9vvzWe6J6amiOuCxKjf/vGhn1nYDVAOAcUm/yQOoozucAOIm8ICQXAOCiA8CBAQAXbQDv0AcweyKAzAX/eX9AAI3knKxIBUUKyTQtgDuSbR8COaAa3AEjYAnCQ1JIbkiCUqAy6B40DdPDSrA7nALXw29QeJQ+KhJVjhpFMyM52mF0O/onRgMTjbmCeYflwNpgU7C3kBxLGUfG1eEWkFwqgeYOLZ7Wi7aGdhPJkurp2ej30U/gbfGtDDIMRUimk4nkNseRbCaLhYOlmFWetZ3Nju0dO4UDy5HPqcDZzeXONc+dwsPD08DrxLvKV87vwL8hUCvoI8Qk1E2IE5YVHhfJFd0phhXrEI+RkJeYlayWCpdWkV6T6ZXNlyPLmysIKawrvlbqUM5U8VFVUaNXm1Hv1ajXzNc6oZ2kQ9GN0AvRDzEINQw0sjNWM+E1BaYfkRNyk8UlyyyrA9bRO8Jtwmz32iXuTLe/5NDqOOS04MLgKu/m6B7vUeLZ57XmLe/jtzt/z3Mim58zKc//TaBIUFBwbchymE54akTfXu7IwKjmaLoYYuyteIGEtkSfJJrk1oORh2VTZo9UppGO858Yyjh2UuuU8hny2ce51nmTBScuSpc0lAlf3lfZVP3+Knud3bWsxtGbis3ZbVB70m10V+49+IHPw/bHhCfHBr49Cxx++3LX6JuxgLdfJ85/2D0tPzszH/2lc/HR1/PflL9XrWz9VPjlvOq4ZruusIHeGNk8vWW5vX9ASM0BDziAMFAEhsARqYckgWwk+t1gDPyEOCA1yAWKg/Kgdug9TAsrw7vgDPgW/BlFQLmhMlH30RBS59qPvoGex0hgiJgizCssN9Ydm4d9hRPA+eGqcUs0ujTHaV7QStIm0g7SydKdoFuk30Xfi9fGX2UQYShk5GbMY+JlKmGWYr7OYswyzBrKBrMVsWuzj3AkcPJydnD5cdNwX+Px4sXwXucj8XPwPxJIEdQV/CF0kxArrCG8KnJb9JiYgziH+JhEhSRFyliaVfqjTIfsebloeScFVUUuxQ2lSeVOlULVZDWiurWGmqaoFoc2vQ5GF9ID+rABzpDBCGO0arxoMmX62uyZea9Ft2WHVZt1244Om/u2g3YTO7874B1FnXSdXZFTzGm3a+7DHqteorvsvQ/6NO6e8RUh7vErJL0MYAm0DToW3B2ygUQ7IaKFvBppGHWcMhwjFhsfN5Cgtq9wPzaJkjxx0OnQgxT91Jajmmmtxw1PPMrwyPyUdey0xplPZyvOhZxXv0CXP1M4cLGjpLH0avmVitqqppquq0/qXlwbbXx+42HTtZaTbb7tsh2Lt+u6gu9a3t/VE9Wb/vjyk9sDo4NLw5gXHCMir+TG1N5qj2u/F/iAmVqaHp/tn2v7XLyQsGS+DH+rXjH48eCX0Wr1Ov0GafP6dvxhQIPUm0SRZ98WqYmlIjWk22AcQiNP/U4oBiqA7kLzMCdS3YmGq+EJlABqF6oINYGWREeib2FoMO6YKswmUllpxQniTuB+0YTTvEee7yE6O7peejv6YbwvfoEhhZGH8QaTM9MKcxGLFcsaawNbOLss+xeOJs5ULmduaR6Y5x1vD18jf5nABcFzQueRc8hVJKojYnPivyTppQjSmjLOslFyZ+VbFSaVWJVtVM6oDqtzarhqntF6rIPW1deL179usGRkalxg8sPMzbzZksdqv/WYjZ5tvt0ve3eHl06BzquuZ9wlPFq8zHc98rHc3emrSCwk0fknB3wLigj+FBoS9ikinPwlMirqW3R8zGpccgLPvrv7o5JFDjw/dCxFM3XyaPox2eM96Xsyvp5MOcV9uiHb9uzsuePnJfN68gMLMUWlxYYlr0pjyhkvV1YaV72tSb4qWHun3r+BtvHaDbebG83lrTZtX9svdBrdnu46dVfgXs4Dpp6TvUyPzvUJPqkaUHna+cxq6NlztxcjI26jA6/Nxm68FX53dHz+vdfkyNSejzMz5NmZOaf5+s+LC4RFvSXTrxrLhOXP39q+U1Z4V7p/EH98/Bn+c+lXzK9Xq0arFWsMa9FrD9aZ1t3XS9ZnNpQ39m10byxtCmxab8Zulmz2bf7cEtty2Nq/Vbk1RI1/dKCqCvXtASC8CVJ+fLu1tSwBAC4bgI0zW1trpVtbG2VIsjEGwJ3w3/+7UMlY5B1TuExFj4QLqLf/+vwP7u7LkNBZmbkAAAGdaVRYdFhNTDpjb20uYWRvYmUueG1wAAAAAAA8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIiB4OnhtcHRrPSJYTVAgQ29yZSA1LjEuMiI+CiAgIDxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+CiAgICAgIDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSIiCiAgICAgICAgICAgIHhtbG5zOmV4aWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20vZXhpZi8xLjAvIj4KICAgICAgICAgPGV4aWY6UGl4ZWxYRGltZW5zaW9uPjY1MTwvZXhpZjpQaXhlbFhEaW1lbnNpb24+CiAgICAgICAgIDxleGlmOlBpeGVsWURpbWVuc2lvbj4yMzA8L2V4aWY6UGl4ZWxZRGltZW5zaW9uPgogICAgICA8L3JkZjpEZXNjcmlwdGlvbj4KICAgPC9yZGY6UkRGPgo8L3g6eG1wbWV0YT4K6DzJcAAAQABJREFUeAHtnWWg5DayhXfCzJwNMzPDhJl5wpxskg1uXiaZMNNMYMLMzMzMzMzMzPi+bO2rp5Xhuvs22N2nf9wry5IsHVk6VaWS3OfPP//8m35CQAgIASEgBIRAyRAYomT1UXWEgBAQAkJACAiBvxAQQ+s9EAJCQAgIASFQRgTE0GXsFdVJCAgBISAEhIAYWu+AEBACQkAICIEyIiCGLmOvqE5CQAgIASEgBMTQegeEgBAQAkJACJQRATF0GXtFdRICQkAICAEhIIbWOyAEhIAQEAJCoIwIiKHL2CuqkxAQAkJACAgBMbTeASEgBISAEBACZURADF3GXlGdhIAQEAJCQAiIofUOCAEhIASEgBAoIwJi6DL2iuokBISAEBACQkAMrXdACAgBISAEhEAZERBDl7FXVCchIASEgBAQAmJovQNCQAgIASEgBMqIgBi6jL2iOgkBISAEhIAQEEPrHRACQkAICAEhUEYExNBl7BXVSQgIASEgBISAGFrvgBAQAkJACAiBMiIghi5jr6hOQkAICAEhIATE0HoHhIAQEAJCQAiUEQExdBl7RXUSAkJACAgBISCG1jsgBISAEBACQqCMCIihy9grqpMQEAJCQAgIATG03gEhIASEgBAQAmVEQAxdxl5RnYSAEBACQkAIiKH1DggBISAEhIAQKCMCQ5WxUqqTEBACQkAICIG6EPjyyy8vvfTSO+6447PPPptnnnmWX355/g455JB1FdbmTH3+/PPPNldBjxcCZUIgGt7LLbfcvPPOW9HhXSZc8+ry3XffXXbZZbfffvsHH3ww22yzLbvssgsttNAwwwyTl0f3hEACgT/++OOQQw45/PDDv/nmm/Dmkksuec011ww33HBhZCXCYuhKdJMq2QoEGN6HHnoow/vrr78On7f44otfe+21ww8/fBipcKMQOPbYYw844ADUnbBAePq2224bY4wxwkiFhUAOAj/88MN666131VVXkWa++ebbeOONJ5lkkhNOOAFuJgaSvv7664ceeuicEsp4Cx1aPyEgBL7//vtVV13VhihK80knnXTzzTevvPLKFrPEEkv8/PPPQqmxCADphhtuaAjPOuusxxxzDKwcxnz11VeNfaJK61QEsL7MOeec9i5ttNFGv/32m7WUd2yWWWax+AsuuKByzf9b5WqsCguBhiMQDm8YIhzeMIcN7/POO6/hz+3mAlGaMWUbtkhCP/74o6GBJQN1x+IPPvjgboZIbS+IwKeffjrppJPaO4MazSsUZjz66KPt1gILLBDGVyIshq5EN6mSTUQgHN7rrrtuNLxR7Gx4YzdrYiW6rGjWEaaddloDlpV+F4kMBtak7dbEE08c3eoynNTcnhH4/fffl1pqKXthxhxzTCS/KM+TTz5pd/n71FNPRXdLfqndVt53CnQjAvDx+uuv/9Zbb9F4Vj1ZE+3Tp08IRN++fe3ywQcfZHiHtxSuG4FNNtnkpZdeIvuII47IgkLkiOeYv/POO3gA1P0UZewGBHBiuOWWW6yl+JFA0lGrxxlnHI/BEubhSgTE0JXoJlWyWQgceOCBrDdb6anDe9xxx/Vnn3vuuR5WoG4EBg4ceMUVV1j2vffee6KJJoqKYpJ1zhbmETi6DBHA/3///fe3GNxHNttss/CuhUcZZRSPfP311z1ciYAYuhLdpEo2BQGG93777WdFs2Ny8803Tz6m0sM72Zy2x9x///39+/e3aswwwww77bRTskqYMUYeeWSLr9yUmmyOYpqEACsg22yzDWYwK58FqcgAZvHffvutV+C9997zcCUCYuhKdJMq2XgEumF4Nx613pXIZMqUCvJWzKBBg1J3v7A06LNq5abU3iGk3DUgcMopp7zyyiuWAQV67rnnTs38xRdfePy7777r4UoExNCV6CZVsvEIhMMbBZpf6jMqPbxTW9TGyLPPPvuZZ56xCkw33XTu4BNVCXrG/cciP//8c9y8owS6FAK8JPvuu6/jsO2223o4Cnz00Uce88knn/z6669+Wf6AGLr8faQaNh4Bhrfbtym94PDG6/uXX35pfG26o0QOlNhrr728rTmYf/jhh56MgNToEA2FDQFOFmI8Whh/w9VWWy0LmRdffNFvYcWJziPyW+UMiKHL2S+qVXMRYHgjTdszRhhhhNVXXz3reeHwxvrKGRpZKRWfj8BRRx31/vvvWxqM2/369ctKH2JOGs5hzUqp+O5EgHNIOCzM287J24xiv4wC0etUrbM/xdBRb+qy8xHonuFdnr5k7Xnw4MFeHw5SzTnRM5pShx12WM+ogBAAgauvvjpcflpzzTVzYHnhhRf8LkQ+0kgj+WX5A2Lo8veRathgBGoa3s8//7w/HunbfYw9UoEiCFx33XUff/yxp1xrrbU8nAyEmHM33PCWTKyYLkTg9NNP91bzhZVlllnGL6MAdq/wxJIZZ5wxSlDySzF0yTtI1Ws8AmeccYYXirm1puGdup3DS1MgC4FwSh1iiCFWWGGFrJTEP/7443537LHHHm+88fxSASHAOTac3+44zD///DlqMQp0uPDsh/h69pIHhip5/VS9CAHcbWwjyuijj67P80XgFLlku8Wtt97qKRneOWox5tZw4blyw9ub2d4Anl833nij12H22WeHd/0yCmC9fPnllz3SP3vgMQp0OQIXXXSR74EGCj/FPRWW++67L4yv3BAWQ4fdV4EwFpsbbriBim655ZZ8W60CNS5ZFS+88MLiw5vjNcLqV254h5VvY5gp1XdPUQ0+FJZTmQceeADLpCcQ5g6FAobAnXfeGULBZjyOqOMQOmwz/LjF+8OPVw7vh4svvjhMXLnXSQwddp/CnY9ATcMbagkRqdzwDivfxvAdd9wRPp15M2dKFeYhVgpHCPDyRHIzB/dGabIuWaKaaaaZsu6WM14MXc5+Ua2aggBidTS8DzrooIJPYnjPPPPMBRMrmSOAxSKyNB555JF+t8eApKIeIeqqBBgRbZmvjlZPOeWUOSvW3333Hf6MnJDD17H4EcBFkS2CdTyogVnE0A0EU0WVHYEnnnii7uE9+eST56xYl73l7avf008/Ha7l11QRnOf9I5U1ZVTiTkXg7rvvDpu266677rDDDmFMGOZLVn4IPPGLLbZYeDcK840WjqQNI9lmHV62Jdxqhn7jjTc4dG344Yfny68sFXAozAcffMA5BoT//u8fn7XJcZdFB+KjdZw18c0333C6E+XgMMXxgcm9laThoAMWJ5CbDFke/dprryEoRafPICtxuCv+fszdQw01FBvmWN8lF+GoS0hGPXmW+blQPl6F/JhHaA4/6mNZ0Bs4ao5vGuIjw3k3NAr1K2dbJ7Xi4CRwoEw+1TD++ONPP/30/nkfyvzpp5/efvttAn7OBh5PRBJDZZLN77FAq2dBlEAbl0hceCiW5iCHTjHFFHSXFVKhvzUN7/PPP3+33Xbz1uUPb09mAb73wFebOBdlwgknjG512+Vdd90VNnm77bYLJ83wFmE+I7jpppt65MILLxyOAo/3AAPt3nvvRfDiMFHkJ+icr3GQK2cO8bwKVBEBPgIbVptRmTPEHnnkkTDxRhttFF5GYfbocxIwlMTINbeJBRdcMErT+suYhJpdg0svvZTpnsl9q622uvzyy5Mf3J1gggnWW2+90UYbLVkTXEjuueeepA6E5xSrC2xaDwfzQw899PDDD0OKwA2vIB/Z2YEMXWdoKAezRug46g+FdPls8GSTTeYxBCiEGWGhhRbCe5CGPPvss+FdPKvXXntt5gg0BvbzQPzhXT5xuOKKKyYdUymQLyzBHHB/mB4WBAdY3yJ5b84555wwgTvHUplFFlnEbxUs0NL3iBJi0FVXXUVLKdYfYQH2wIB5tXbCRMN70UUXzRnejz76aNjk/OHtKXnN+OAxn9nhrdtll11yyvcsnR2IMOddzcEk3GcFLBtvvHEOOLyWfI4smoVJj684shETbk5e3aooAvZZca981tcySMCKNVOrp5x66qnnm28+v0wGSMCP+NNOO+3NN98kgKiXTNbimCFa/Dx7HNjhUmv0zIZUdNZRRx3VbqFSH3/88QZQWDeWsq6//nqjZ1gWYsDq6ETOWL3yyivD9B5G0Tz55JOTR/sygZ555plOz6i/FMinao3mycUp/+EZC14gNgCMJ0bPfBscacO+zwOZXXLJJai2CGLQM8LBv40C/7lrDofRgcM059RTT0XJgJ7xQmTZgwpYaVTvrLPOQkH35xYJ1F1gKkp0E0IJVkroGczR7KeaaiqmV3OYxEhADat1ImM0vLO+lgHUCNHhnksavsACC2R1ATIiZxDyIUW2ViPVsbZN92Ul7rb4CHO+QZSDwE033eR3mRNWXXVVv4wCyNZzzDEHOJ944okYeFieRDCyGRaVGndxvvYdZdFl1RFgIgq/Rkp3J82H3kZEw3AndHEJ29gHRphzzjm9tHYFWq1DWzuZ3PlhleJoIejZSBE0r732WnagsuUXAyMLDG4WxhhrKiNZ0IDJ4regWMge1uQcIm4ZeYRooryiK2OyRrKG1+FUu8sIJ54wUwa/scYayyxjcNVll11GNSiTySV5nhGqOYRKacstt5yZtakwTAabcpwk0gB1WHrppZnQrV1E4p6KhRzCQ0UIz2qgDsbBUAVZrFG8hagFTFVkRIZgaQQ7NqaFAQMGUFuebsyx4YYbQufEGKN7owoWaOn9bypKLAqwNECaSSeddJ111vFV2O+//56eQkZBIMCqsfLKK3s5ZQ5EwxvSzRneWBfC1dP84U1fmMkE8aVv376h5F5mQFpQN0YKb5E/CJk1R4F+9dVXw/mXVw5TlucNA0i6m2yyCStBQM0Skt3Cp4wsHLFu+O+xxx50cc6J62GBClcCAfQBZkWvao6ETRq3MhJmTmaQesacgO/1QOEuw4ET7dGhAYjpHkM3aqubphGZ11133dlmm427aJyhz63xBPFrrLEGcpPTMzHTTDMNZEkAFTbSUC0StoOV4TmIhB51w7XNBZiRMT5DgUbPZGFSWGWVVQjwS1VhmXSQrRj5vurM0rX7FHCXpZFw/YzarrTSSlZ+WEMq8Nxzz/EUzOYk8EbxMiExYGPnFo0yIyEo8RR+/tJQT4txhq6pwL+a938/npKKkk+XtMjpmUwsRSNasV5O2Lvm/wor7/+6hzd9t8EGG+Q0DJs2rID/J6aaaDEiJ1c33ELKCb8dyYud0+pwSiVZjol7zz33BG1I2segFcscgl3HhhIjceutt0YsznmiblULgcg0lc/QKFreOpY8kA79MifgDI2onZOsZbfaxtAswuPkFbUTcsJOaC5amLVhDkuA0ga+qIxoz1EWLt3WjS4b3WWUMr3CdpEqjL6O/kSZyYVhSnAi5LlRgVxSveR3bd32SxOS/gW01Lz8UTq9QDvZivjUBTNc1WzrHmsBjoPnTQ3UXWAWSi40JNcIaCZyDFYE1oHQTVPrU7bIaHjnswV+Bl5/BBR3CPDIMIDHAGlcmQtvdXk4wjx/SmWRyOFC8s7pILzDSLnjjjviSeBZLMCgdm0JFsf1LEqgy+oigIEzrDw+wuFlGH7ssccwyXgMspqH8wPu2NjVDI1GmGXih7GMmbBm+Oc/4bB//OMfoOz6oqOMYsQSlF8mAzhRJ6dOZG0K5JecBeAb1NYcUsTgjBIZPQg5AF4nkmclncCJj4R9nmL6NDp9slFWOJMUAXCIVvLsbvS3lwWmooRbrD0FuzpOALYo4M+leljy+cHWHlnmQPHhzUImqxLeluLD27MoYAgUxxx7Fa6gjlsO5ujlPvli+6GzPJcFwrkFd9TobuUu8YZhoT3LGJB/l8b2mKBCgER+qUzFWZW/4IIL/BaTvHsHe2RqgP0ytgiNhTJJDalZmh3ZnnVotN6sFSYa7Poua3vJVSvGPN5kkDeL0yxmM7DzdTjjuRwc0SCRtSmKMnmb0RfDBchkRmPiZLzFZLXLrNyeC7azIUdbsuyiPrtF1OiFhIFeFpiKkm36YnYAYWZPfqzWsyaNQomKj5QTVqD84fqGN2qf1jLr7lxeoTBvzpSKNwkj0RKz+BXtTA0L4cXDlmabD5H1fbrwNGT3cLiu5JEVCmy77bb2IWREeQ5ii85vyb9LM3tMUCEoqCpdD3f4/Jz1OmH7ZLHDmzZo0CAP5wfcxA09uwXRsjAb4wPENItuneO/kl9+HXfbw9D5k7tbv8MNSyi1GC7wtIJEfSTTYBRWaMN3CSchyHkW+48xl+HJEi6VUQLL0qjmWUJr8hF1xLh5gDbyyy8h1dgeZellgVkoYb9l0xE6tJkrEWX40RE8ncmXlxU9uyo6NG3k1aJnDbqc4R1+/IrhHUlXEfK6zEEAkxhWJX/DkwK35WWsGQ/Z5WGHHeb+FsnC6Y6BAwfiCMZMnbqfjSnCc7lnqMdUKMCZ0g4Luh2DMfwuZ/5dmtljggpB4VVFUjGjKYZMc4XxWx5g/Powx2Mmf5OV5yLgJu5o9QT/3/3228+kPQR9jJpZE2ZYWkPC7WFo3KRzau8WZnciYws1GJnoRCQ6HIIzI5/hx5jHqZgv/mYVmNWLiEu+nQa4kcphen6UTB8ccsgh0RJaVvn1xbuARhP45ReStNIn0/eywCyUeNBcc82F2RDTAoZfpgm2k5nswvuKjzpuehVSMTEA2NDNGd74z/vwxjORj18l0VZMcQSYUo2hEaaRfVMz4tTjXpm4cQB7ajKP5JXDbsl7mLpCZBKkJbb9V56xWgE2FIQVxpqFA41zQ/5dMvaYICy8KmGGsDF06mIircAb6YgjjrDmMJkfe+yxxZvmDO0nTKC84SUKWRxwwAH4SKGaE4Bu3NeheOH1pWwPQ7uZIrXSvonNmYnNV5YFtlh22WWdjVKzR5GpHckeLaNnmIljN0KzWJS9SZc+VfFonMl7/5ReFpiKktcKrQU3PX7EsFEYH2+W6m2lllVApmBzp/f0pQ3UNLyRnAYPHlzatlSlYmBulMk7lmqNgGj94wfo3OGXpHPaSFGp9EwWP2qGBP369csppOS32C0W1hC+cXomPv9ukQRh4VUJ8zpZVaEJHG/DPSYWf/DBB6NFEOYNYRMsoFl8j385AtKWTlipNJdGjJdMzsx4bL5lgoV97EQdRISWMXR7fHxA1g1fSeBAyiLN3I8ua77EqMv4DyfpOdwhlywtNcZ9rygwSc8su7oen5q995G8WLZinYMDdcB6zy8ywqc+veEF8hQWXfhFtgRsGOgl7A92T4oKbbgKhzeGmSSS2E5MmbPhHS1dJ9MrpkcEHHNW8niZk+kRg9x466eOJJMVjDGHCUu8xRZbID4WzFjCZJyYhjnBTInAGC6+UNv8u0USlLDJPVYp3PnidhfPxc4X/y4Lh8vmfzrac1nAF6GhZ1gGkrITL9gOgLDOwQ9+4B0bN6K8zbtsD0PTnuhzN95CGMuOOEC7Nb3Qt/okqdRyuWOnF9JjgBVoS5M6gBHBms3QPN0ODKexqdMWCYCIjbb8Urkk2caGF8iMwIJflk7j5t+C1UtWuPUx4ZeJk8Ob09PcPsaJ3Mk9dbVWOHSYqDVvx6TPx5z3f5999rHGciI3S629aTjqDud+G+ycrOC92Zsy25iXxXh2/fGiYn1FDoYwwsrk3yVljwnC0qoSxtTsO5vdKG2VR51AoDGFjc30bpgp2DQvDfca5mREAVREjqo0nRAJyWw2nOscSgkFC687WdsYGnmEw+6jeiNl+zZzVgLM9Oq+06lMhrnVD9ZgfEYFZl363qdkmVg28Jm0jMULzHpQTjwzFz5WzCa8BMmd3Fj1cR0kOw5ZoaHG3bKSa/n1FZhTQxNfgChVS8b73fK6kpRTVEluMbzNVk99fEBa3Vh79uGNhaD4VylzmiaGBhz0GH9DXE0x0DDPgLkdEoA5kfM7c8AscgsV3BYp6eVrrrnGp44ieUubBpdGOCN1gYA6598tkqC0DU+tGBYFPrVgt5DAzCeGSzbF4N5lXMD4Ra/wqTK1nGSkf1YHmyUHyrLZGvHIPRZxVEKv4wWGuVr5XrWHoUEZ8sPVkGMv0YBBBNWZFU2OLDfNBp9b39QIRRnWLB5zADKCEnmZT7nk6GxK8J4gr+9QSnZAGOM+pdSBchC7+KGso7byQVDcla1MaJJfk/RpLAS8B9SKamPoY/EM3yueBR1y5CeHe5t5ObKouEsX6+hsBmU+cs/V+goMYYnC/rVz9oNxZhZUjVgA+HQBgpHLUtF6WFRIqS7p1r322suqhDXMpRwb3ma8YYUJ44G/VKWqfxUrA5KuJeMY7yOUUQZ5Y7egUdAzr5PPhvU1E1O5fTULYxJ7NFzTqq805SotAixecAgj1WPZmMMccSLmiGhUW1Q+lDpetjrGry9CUyzEzzTLRy1tfcFxYIJFb2wlPfPo9niKQbqI1dAhEz0/h8ACDC38OxwdbAucWW0nAnIUaHgaKOnx6+Y4Tz62weIxQhDuiy5hRcWGl5yzYZ+thWyi7cg8l/kCYmaQm1sgxiXSh9kbFeaMbh5HnTEU8wmpZLG4xXH4cBiPcgAgUAumeH7cYppzJbuOAsPCozDrzbQdqYjH3fHvX5QAuZ6FfNdKo7vlvGR4c/oK3pgMb+DCJRhiRh6iCxje7OFhhDeKnqVD2zuAyRHM4WDsLrxRbIABfCZW5GygZirkKxe9pGfER3oToZZFRKR2NmWU8/VTrRqCAMfPsdGOaZ9FYn5WJkodA9nOja71Ka5A46vIdIfbPOoTbijIfGzNN/t2rWU2JH17GJqqY2+Ep5n2Q1cp7P4QEpRj9m1vIRswGMlQZrjkiZhDZ7AkAMnNOOOMZjPPsgV5URZgsw2H+vK5CJz0fBrloZiMoGf+YtDAyGzac8Eyo0cUueSJPI5vLWM8YYpxiw15WXSnaW4e9NJoNeILFlo2QCc9yOoo0EtODSCaMNnhfWP6pacBcw45wVmMyntkVQIYTho7vLMa7q9WVoLuiWc7BpL30UcfzavOzxrOsIWn3aGhbjQgZnQp1CDkLb4752tYdReojCVHgLXh4447jmNf2SaA2y+7fniLzCRZX82doZkZdt99d6yYcDMl8wheVxYim8cC+RXu0+JJhLMIYFnGKiduWs1gaOxdzPhYnvOHFvZVUqLdwkOwe2RtgOFYQqaQmoRxxjYVICM8xC9UnpCkGPMIDb7pKx/KXt611qHTw8E8NB+KIs9qbIGYE9B46Dsgwm+cGmadnlakbmVIg8zRqOEdNgeDua+hYB2xbRthgm4Os5+e1RzEYk6GAhnfDtAbTBin2DyZUjk/i82v4RC2ya1dc2tvGqW8LUYATcO8bfDZ5lNGPJ3pDsrnjSWMBReVssVVsse1TYf21sJ/BSkQFsec6xZdL8ECPZ77EaW3S2wa/FJvYdlo5QprfutSa5gf2dgCkYf4OffkP7oSd1mt5NfUqrZY/G1qWxpSOL6H5n7YkNIoBIQ322wz6JnliaRzH6fNc+Yd5+o06nEqpyMRwGJq9MwU5yI1vlBIfmylocl4hxlDY+ZkmQwrZu83ehREsj2eYgUrp2RCoNIIiKGb3X24CHEqBbvYk/TMEhVOoDgQNbsOKr/qCLiJO/omNLZVa5rrkCwvcqxYTWbaXoLTfh26lw1QdiEgBLoTAbZd4ByOu1DqZzZwnsAA3kozWHf2Qge0Gp9lawW72sLm+IZSP4qDjbj4z/qZoGHiJoXF0E0CVsUKgb9ssEKhSQiYNy/+QRgeIWmWn0GbDR1coj2j/bAHgUeLoZuEfycVm8XQvmHPAuyG5aOWvl2zNQiIoVuDs57SLQjgYOJNDfcpeKQCvUcAsyTb1qHkf+++vD+nQDF0Dji6BQJ4v9qhs+EitCHDPl7OwCGMowNuSXg347mCQ2IrcWu1Lzd2A4Rc7Pj1OXa1Eho9SwgURGDAgAEcCIxNleOx2Gfvu+Bw1mOPJi4nBNi4718SLFiskqUigAyE1ZE9Hal3w0iWD9n9EcYoLAQiBDgwCkduWGnnnXfmkOPoLqcj8CkO7hLPEEYidMU6Stmky1YzdJOaoWKFQBsR4LxVds+z/Yx9gJhb2d5jO3wwuqLnwdzYXRnefCa1jZXsmEfjdltwFz67+d2A2THNV0MajgCDl223HBoabtXzp3C6Dr7c3OKgDg7S8PjWBMTQrcFZTxECQkAICAEhUBsC2m1VG15KLQSEgBAQAkKgNQiIoVuDs54iBISAEBACQqA2BMTQteGl1EJACAgBISAEWoOAGLo1OOspQkAICAEhIARqQ0AMXRteSt2FCPABFXPPJtCFzW9XkwV7u5Dv1OdW8Y0SQ3fq26h2CQEhIASEQLUREENXu/9UeyEgBISAEOhUBMTQndqzapcQEAJCQAhUGwExdLX7T7UXAkJACAiBTkVADN2pPat2CQEhIASEQLUREENXu/9UeyEgBISAEOhUBMTQndqzapcQEAJCQAhUGwExdLX7T7UXAkJACAiBTkVADN2pPat2CQEhIASEQLUREENXu/9UeyEgBISAEOhUBMTQndqzapcQEAJCQAhUGwExdLX7T7UXAkJACAiBTkVADN2pPat2CQEhIASEQLUREENXu/9UeyEgBISAEOhUBMTQndqzapcQEAJCQAhUGwExdLX7T7UXAkJACAiBTkVADN2pPat2CQEhIASEQLUREENXu/9UeyEgBISAEOhUBMTQndqzapcQEAJCQAhUGwExdLX7T7UXAkJACAiBTkVADN2pPat2CQEhIASEQLUREENXu/9UeyEgBISAEOhUBMTQndqzapcQEAJCQAhUGwExdLX7T7UXAkJACAiBTkVADN2pPat2CQEhIASEQLUREENXu/9UeyEgBISAEOhUBMTQndqzapcQEAJCQAhUGwExdLX7T7UXAkJACAiBTkVADN2pPat2CQEhIASEQLUREENXu/9UeyEgBISAEOhUBMTQndqzapcQEAJCQAhUGwExdLX7T7UXAkJACAiBTkVADN2pPat2CQEhIASEQLUREENXu/9UeyEgBISAEOhUBMTQndqzapcQEAJCQAhUGwExdLX7T7UXAkJACAiBTkVADN2pPat2CQEhIASEQLUREENXu/9UeyEgBISAEOhUBMTQndqzapcQEAJCQAhUGwExdLX7T7UXAkJACAiBTkVADN2pPat2CQEhIASEQLUREENXu/9UeyEgBISAEOhUBMTQndqzapcQEAJCQAhUGwExdLX7T7UXAkJACAiBTkVADN2pPat2CQEhIASEQLUREENXu/9UeyEgBISAEOhUBMTQndqzapcQEAJCQAhUGwExdLX7T7UXAkJACAiBTkVADN2pPat29RaBwYMHP/PMM8lSiORWMl4xDUFAsDcERhXiCFT6jRJDez8qIAT+C4GZZppp1llnXWONNV566SW7QYBLIrn1X0l10TgEBHvjsFRJfyFQ6Teqz59//qluFAJCIBWBRRZZ5O677+7T5z/DxAJE3nnnnanpFdkQBAR7Q2BUIY5Add8o6dDeiQoIgRiBfffdlyiXYi2wzz77xOl03VAEBHtD4VRhf6vuGyWG1usrBDIRQPTu27dveJtLIsMYhRuOgGBvOKRdXmB13ygxdJe/ump+DwiY9O2JokuPV6CxCEQ4R5eNfZZK6wYEolcouiwtAmLo0naNKlYKBELpWwp0y7pEsLcM6i55UEXfKDF0l7yfamb9CLi47YH6y1LOwgg42h4onFUJhUAKAv4ieSAlUcmi5Mtdsg5RdUqJAAI49brrrrtKWbuOrZRg79iubVPDKvdGDdUmoPRYIVAlBCokdFcJ1p7qKth7Qkj3a0Ogcm+UdOjaOliphYAQEAJCQAi0BgGtQ7cGZz1FCAgBISAEhEBtCIiha8NLqYWAEBACQkAItAYBMXRrcNZThIAQEAJCQAjUhoAYuja8lFoICAEhIASEQGsQEEO3Bmc9RQgIASEgBIRAbQiIoWvDS6mFgBAQAkJACLQGATF0a3DWU4SAEBACQkAI1IaAGLo2vJRaCAgBISAEhEBrEBBDtwZnPUUICAEhIASEQG0IiKFrw0uphYAQEAJCQAi0BgExdGtw1lOEgBAQAkJACNSGgBi6NryUWggIASEgBIRAaxAQQ7cGZz1FCAgBISAEhEBtCIiha8NLqYWAEBACQkAItAYBMXRrcNZThIAQEAJCQAjUhoAYuja8lFoICAEhIASEQGsQEEO3Bmc9RQgIASEgBIRAbQiIoWvDS6mFgBAQAkJACLQGATF0a3DWU4SAEBACQkAI1IaAGLo2vJRaCAgBISAEhEBrEBBDtwZnPaVECHz22Wf9+/dfYIEFJptssqWWWuq444777bff8uv3+++/P/jgg08++WR+Mt1NReCLL74YMGDAwgsvPOmkky6xxBIDBw785ZdfUlN65J9//vnUU0898MADf/zxh0cqIAS6DgFGgn5CoHsQuPvuu8cff/xonM8777xwRhYIP/3002KLLWZZBg8enJVM8akI3H///X//+98jwGeaaabvv/8+NT2RsHK/fv0sy0477ZSVTPFC4Oeff+5sEKRDR1OHLjsZgddee22llVb66KOPdtlll5dffvnOO++01j700EOHHXZYVstPP/30O+64w+7ee++9WckUn0TgzTffXGGFFd57773tt9/+xRdffOSRRyzNs88+u/feeyfTW8y111574YUXWliAZ6HUzfEvvfTSGmusMe644w4//PBYwnbcccevvvoqHxASnHvuuZdeeimknp+yXHc7WwBR64SAI/DDDz/MPPPMDL8jjzzSIr/++msfjfPNN5+njALLL7+8J4PIo7u6zELgxx9/nG222YDuwAMPtDThasK0006blXGrrbZywAlnJVN8dyKA9DbCCCPwhgw11FDDDjusvSrw9DfffJMFyJdffjnjjDNayv322y8rWQnjpUP7VKBAhyNwyCGHPPPMM4svvvjOO+9sTX3//fe9zajUHg4Dv/76K4Zxj5l//vk9rEA+AkgzrNyz/Lz77rtbyg8++MCzvPrqq0yIfhkGbr31Vr/EXcDDCgiBRx99dKONNsIvBPcRKBlnBcMEaw3Um4XPqaee+txzz9ndV155JStZCeOHKmGdVCUh0HAEMG2ddNJJFHviiSf26dPHyncrN5fJxWlL8/DDD3/33XcWHmaYYeacc04L628+AqzrAzVp+DvEEP/RBHyxgHhMlN4RYVFMtW+88YbHiKEdCgVwOVxzzTV5tbBXr7/++gAy3njjOSx33XWXh6PATTfd5DFzzTWXh8sfkA5d/j5SDRuAwEUXXfTpp58uuOCCU001lRd3wQUXeHjJJZf0cBi4/fbb/RJ6Hm644fxSgRwELr744o8//nieeeaZfvrpPVmtgE8wwQSTTz65Z1egyxHYf//933777bXWWsvoGTTeeecdxwTZzsNhAE9P3BU9plpmMDG0d5wCnYzAaaedRvOwj3kjUdTCcRve8jQEQrUPg214S+EcBAzwjTfe2NNg4r7tttv8MrzlkQQEeIiGwo4ApqwzzzwTe8ygQYM8MlwQmXjiiT0+DDDM3TsMz7JZZ501vFvysKzcJe8gVa8xCKy88spowL6Hh0IxlHnRbP5JHbf4OuHm7cnE0A5FjwF85nETW2+99Tzl+eef75ubJ5lkkr59+/qtMBAuPQjwEJkuD5999tksPHOAwYQTTuhQhFaZZZdd1uPDQCjzsa9y6KGHDu+WPFwKhka4xnsepGaZZZYxxxyz5JCVqnpsX0G0xLORN69UFStbZf71r39FVQoZ2o1mURqkbz9bY8ghh9SaaIRPziX72aK755xzjsesu+66qYvQL7zwAnvhPJkY2qFQAIYGhA033NCheP7555944gm/LGIGW2ihhTx9JQJlYWhb7UM4EkMn3xsWUFE+cFMaffTRo7scdPXJJ58Amhg6Qib/Es349ddftzRQRahbhxlDfQ4le5RRRgnvKlwcgaefftqdacmVJRKFgI899tgzzDBD8UcoZWcjsO2223744Ydsg/ZmnnfeeR7G/2uaaabxSw98++23jz32mF+KoR0KBRqGAN6wrKNMMcUUm266acMK7e6CLrnkEgeAndATTTSRX4aBe+65xy8XWWQRDxcJoAsee+yxbDQaeeSRi6Tv7DQ4jnkD2Zkauo95PIFwY1uWGTxMTxj/IBQpjkBBfmWOpvDkEWZRFl1WEYFIRUZpYd3EGxKup3gkgfvuu8934WPfZrCHd6Mw26avvvpqTgX+/PPP+ctvyimnPOKII6JkrbwshQ7dygZ32LPYbIBuPeqoo3ZYu5rdnCuuuMIfsfrqq3s4DLAI7WdgEV+QMEjJugMWOfxOMW9svvnmYmgwufzyyx3bLMBJUBNDs519m222ueaaa7xkAlhE1llnnYMOOogjLMJ4hTsMAaj33XfftUaxAkWnpzYwtMrMMcccI444YmoyizzrrLP8sASLYTkmJ30LbomhWwByEx+x9tprN7H0Di2axU72bHjjVlxxRQ+HASzhvgiNB2m+fYyTE9j4wSkcFH7ZZZfh0hIW1eVh3ObDYyKyAOdYUGQax2rRRRf1cDKAeYNykE333HNPhCekIvrrlFNOQQ3izCkEAs53xFstmVExnYFAaAZjbLK9PrVdoRmsRyGbN4oVQ2zpe+yxh3k1sj8ztdiWRYqhWwa1HlQWBJC+vSrs0Ah3SHs8gVD6ZhF6tNFGC+9GYWzgdoYoJg3WI8TQIT7h2drMgHYUaJjAwuGJE+OMM07OIjQ0jFWThWr6yFcoVlllFSwWsDJMj2iF9oOjH86nyQcppgMQuPLKK70Vq622mofDAF9nefzxxz2mR4bGps2P9EcddRS7+Qm03VexiQzNMcg4afPDmj/GGGNw+AC/kUYayfHKCrDBHPsVGfGQGmussVhVwoPMz19N5kLY4YsInM7P2ejYuHgEj2N45xwuwaFxdABZeArqEZMv6ZNOaiSjZJ7IfGFeWjQKbYmjbThIgVNhObaQJ0499dTJWhFDxVCqCOBr7fOIpWSdA62COR1HBsohAftPeDkIe1FUjzfMyuEvj7aTKcMnvvXWWyxRwwqpNj0qAIZ/9cEHHxAGf07OQtjEKORP8QD1oTlAx8RHJByDomm6Js2nF8juicNAHfiH2VsfxmvJH5rjYRcSRr4+R2kI3eADH3DcNJ2S9Ur4c7sqwGGr3l7OMPEjxjzSAqFIlD+ZojczOliGDDfeUAgjCI9xOzSKsbPJJpuEvr7R43RZXQR4o5jTvP58ncXDYYCvl/oiNJNeQYWYadboGUbIcpgIn9LU8P/zQQMfA7GxkTwUnL1w9qSCZs6ONJYWWAyApD0LAYb0Msssk7rXhW1a1113HTJ1mJ4wn8fBKWDppZeO4rnEhcdOmApv3XzzzaxSIIaH0wfVsC0iyy23HE/nAMgbbrjBuhxOhX3tRBsWw6KZwkpmprbsTDfO0LA7FU49BRqRAh9X51qOdzCCt9KwvVhp1PCAAw6wSPwazJc7Wj7hLlIO7jmIApbS/+KQzMl5yaOa2H0E12IvYschRkI/8NYzchYPeIYyBLfqwN8LbFcgfFuyRiCL0HS317BHN7H/+Z//8cQKRAjwzntMFuBIh8VFItv6gRrNgkLoUsBTmGHYF3vjjTcS5lRwFh2ynuhVUqByCIRnDTFh+pwZNSR8o5jeC3qEeC4mQ9ShqMwWXw7R8Ocx/bEa5PQMk/HZdocGx/fjjz8+XG0KK8B61RlnnGH0jPwCq5m2B3NAjSwsuUBkueA5dqzbhIvqTD9BPLYlhg8esAJxyy23hOUT5uB1XKPRLAlb3XwLE/YQ1jZ4VpTFLlnlwiclrMDss89ut/AjTc3i20vcrGfH4jg9c8ANFfZm0nA8jEx8Sy2weCTC48knn2z0DKOjOiNDGJgox4Acnu4UFYv5yOiZRT6qh1ZtCSgzPMGHyDrwj57Vlsvwgw1ZniM01hehwa3txq62ANWoh4aAM+hSi2Vvq41Ku+sf5E4mxlTmq9q8q+Eh3pY4FOXD1cpkUYqpKALhlFvQDNajkO1QOEPnG3I8fVMDjdehEWkxQFFpKJYDVH3nA8QDxaIIMg4ZNuidobZqjbQFQhDHqGj2cBgRLQ2VjukS2sAAGwKNKI2+Tjl4h2JgNHmH6QDm4BGYf3ENRfNz0zraM0onCTBrk8X1SGR8qs22DToec/qqq64agY4QYJvq4DnMmFh9Wb/Etow2TA1hYlT8KAtPQX4nkixmNyZMetMnaCM/bPhWZ7gZbYD1MwQL2mteD3gnmriAuz/NR9DJ2lEQPRrd/frrr7dIqIXJzowWVBXZE26mWCyK0HZyqQ/vZXDDSAsIvv2XimF1IDtqJQU6q9WKf1TPdl2GJBF+gNLrA9p8CMsvkb4dCo9UoDgC/sKQJRVwXkhc371A1gtS97ZaAoYzA8qkfISn5PoXfgBeVHhus0cqUHUEwq9BMyGnNodlDvQxvxUSh0emBkrF0A3WoWEXE2nhMDaYOz0DBKyz9dZb29Gp8HTSiGpgIbbgUOecik2VDY58I9Y4BtXcNGwSY4e0pQh0UBx53BxBgD7zE+BCERvTN8SJMZkCnZ4pCjWRrcasBBNm4Srp5gMzwU98uJAmQHhUiSmbcqabbjqywN8mlBD2H1MDa8xcugJN2E7JAATayCzjdaYoDOyW1+cUIuESfpaMycgu+etPSQZoIPYGi8eBAru0rykAJvA6zWPYR76JSoCemRzZehhyEs00T2YECN/hUAf+0bPadckJoP5oVDcPWwC24Nwis6NaTI4+F+XVZSoCoU91EnCy/POf/0R897z5q/68xnxwmuED8ffv3z+5wGRGNSst6VziT1Gguggwy3nls+ZDzGDMV5aMd6bgIjQUZqfaocWVwc2wwQzttlMIMinbAtPyyy9vkHlKB5oAWKcOTnb9mvcH9MwpWpYFxuJHGGnaDZJeGtohi8f8fK8wljGUSxIw4Yb0Y1lQxE11ZoJOepcQiRBAxkjvzzF0m4mb9DPPPLM9Au0BUQCpJbXjaTtKOSnNO8yy1PGXN8w+e4zggvKXLAHxhR/x2MBZqEsmcOEmvOUCjetAteIfltbeMC+hy45IMyDm9UE254sO4fEa3Ep9Jz2LAj0iwHfDXK/FeBO+dbzt0PMJJ5wQFoIoHF4mw1tssQUyMYI+VJ28G54hleWon8ylmAoh4CocdQ71aW8Cmkb4buCd4CutniY14Ao0Okk026O5odVg0PU5MLWExkY2kqHRMk36QLzN8mVlZsRaSxtoJIuyUWPA0RW+6JafBePaKnxmww9ljhGOJZYKeC4Ij+UofqYZE2+8RSCrblTb1qTDGcQKRItNXZPAd9Q63pecLT0inukKPMtNfMgK//j3D/u2JfO/SABIHkk5wxMUD3gzaXtWLr/liT0llQQHv/SAC6ouQNSKvxfV9gCyxVVXXWUdx0hGuIYkjjzySOwo9Bcec27boKo00+wHba92dSvATMcqkg0uzDZIPCxyDRw4cMsttwTw4447LgScZvbI0NYvWXOuMzR+Hhirqoubap6FQGgGi+ZesjCdstcu3AldXMh2hg6t4hR4+OGHw1ysZnLsKIqZa+dZNWxUfCPXoTEumfEBE3dO/SAA02XZhRWKQmQJP8cdlYD2yVwJh5HLb8E0LB6jWLO2zRmtsDsgYkPGZYy/pmF7YndPMz9Pjw8DzNdchlYyu8vTU3mLqQfdGts7chzmX9RWS48YYXJWaOIOHwRQNASBhprbvq9USTDMUjDs+OT0grfFE3vhPVoFQ/tSTfj7I8oQwLqAHRsHbAYktgRIwmqF4YfVGdroWh1+DEz0ddc5hKvuQjogI5YkPjEE4DgbMjTw1rRGYVdjTxRLYIceeqjFIPXaWlh9rcb1xDd3IXjhbFFfOeXJxfzAK4ooA1bJWuXfJX2PCZJllj8GOx8viS0I3nTTTahD7lKDow/9jkQYtqL4QpWfaucMjeqM0xJORccccwx62mGHHcbkwOyR9D0Kn9iocEqX1120T/f5szzuUfYI0ps+7U90i7THhAEWBmBZ6BOJxuwPMPH222+P85ft5UWuYXsSP3JB57Ajbk3kskLcU9RdqcPCwzBiPqzPMrBH5lSMp5jjOl5mztCmQFOC2ZO9HAKoraRnmzWqfxgPa9K00AwQ3i0e9l5AqsjKhfIBFSGOeGJPCW4e7jFQE/49ltbiBKybmMWV7dH27Xeaw7I9c/rcc8/tlVliiSU8rEBvEECWZW8FwwQbFYDzqgM46jJWLl/8onxM4nU/hTK32247y04n7rfffnUXVZKMyIsmLIIVrAOGYcXy75KyxwRhaRUKo33hxo9mDH0yjzHP41fLiwSP4o2LQINVxoVj5jq3Gua30RehmfANauZkdp8yJ2Cjtfnc5gqkwOoxtBuo8y0AftfTO2p+y2PCgBmB4WajZ7sFlPj1cJoHJA3toZ2bBkxi3LvwR2PJyuRoXxfHzyssNjUcMWUOQyP+46sC72JsQbIze50ZXvjqcCT2Qgm+AE+ZvFLIK/woBPsB/sNJy39q9XIiHVWa4OEoPSKONTArQZQ+57I4/jmFtPEWAha/sAIMcncBpTfZOx7erTXs00StGTs1PYOCX9g6lpPDXXxZByyHWbLCgwcPtqHHyGIhIxSys7KUOR5/CLflQAz4eIZ+dvl3aVePCcrc9h7rhqnZzGBMqujNDhSdjpTGFOcxrJAWNIO5iRtGh2VgekR2W/lCJmAF2uiZuhVXyntsSH6CRurQMI09jIblPNXvenpPnGPpZaYz1+hU1ZBVUkQek3qwobGzGXqG7RCvOHjIvg3s1nXWMHxV1R+dH4iINkrMFA9D81wM3dheCJuUEE396O5Gz/ippZ4ZEhVb3yWomvEH/dh1+qgocDYv7mQXRCkLXhbBv2BRTU2GaYTN+kxzaMa8BqkGA7d4UxNWMRmfvalSlzM0A/C0005D4UDdwRPTpeQQUizeLprjiYI+FN4tHkY732uvvUhPlzHQOsC+zRkMYfORHZlkXFvIv0vGHhOEhVcxzNvC0gkmGTQxuBN5GksDKi+79eozg7mJm5eQyRxHY+QATo+wBVPcnggwc0L5qU64zcCwkQzNqwOToZwhFNMMa1Wy0hCYRSbpAZTNZzuZC7yMVJyhsRKboRjCCxmUaiDggCCuKGSBLJGwyOXrsogIWQxNYiYLpKdk3ZJV8hh8s1nb5lnMETC0ybk8EUHe0xBwn2E2VrlrtCdA6GuIp5jXnK1oWQyd0wVenx4DteLfY4HNTkCF8dGzRUp4GubYbLPNoofybtjBbRbf+5PCupmhkYdYxTe/SwBHwXUTtMOO9Yujdfxyt91283BNAVZbEadwY2T2ZFNl/kJbTSW3MXHkiI6lzemZWuXfLZKgjU1r4KP/MoL9txkMi3R9ZjBnaCgD+keP4gOybrLFlwIHI7bvQlJmK21gK7KKaqQvN5XGy4MnoeyGn+0Ln838aB5bMFlSoIbhEBLD9B72Au0RxNMNcDA/Zz5PTIBX2a3Zpnyz38NgDU+MC7NAkPQH7gDJk8jCZMkwfG8HLGBhY0Y2O5sp9GFi50UEvTDewoggDWFoxweLjasm4eMQBRA8LaY3CmKt+Id1aEuY0/CNnu3p0VcLLRIKcWd11vAKLl+1pTnlfyhDKdwWgb9Iss7IQO7CiT8OjrLJND3GYC1D3eEMebJj+ewMeqbVfAiEFpmqg8sOylwIRf7dHrOHRVUozCSJ0YWxyTp06vxGWzi20luEi0PBWQ4HJnRLy4jnNmuXSPARE7MWidDZ+8VBr16PgUYyNA9j8dwkDsaJE5JXAl8t96NO3XQLeXBwgWnGnosAE6t97QDeRbSxW85zkLep12EWmBKxmhjUazvTAAnUNiIjB4TfTvBcUJpxZJJcPU1WwIQ43BaojJnxk4X4WohPSV4arODOh8m2GKRZr6MXYgEaa82kMjhNRKWBMPKHVQAZ3Ok8KqTIZa34FymzqWlcyLOnJPe88e5xdJrd5TDnhny5vZt16B4BZ/nZFwvZh4meXccLwLiAxjjDAA9eThL0UVZHUWXLwioM7ySLVkxNqG5IIWEN8++SsscEYWmVCGOVwUbCVj1eG45jYv9OstrYQTk72eN33XVXD+cHfBHaHGkRLjFx81oiEESzaH45jb3bYIZmrdcWAGBZjFfoavA0QwjZBAdmRBtogwbgKoIOnWwJ0iKG7pNOOokdjbAIi1i8l2wSZ+BBLaTHfO0GbbZXmhWXg7qQLlHpEKVJRi+SC1ulbc5GdfYsrD6aQEqBLPvjXEYWFmUxidDZZuKgS3JOHEzW2WLIYpZzE0FoXVKQN0GB9HhwsCZN6/hhNuGsU9Q7lo2NiakPv1CftvNVSIncwxJ7qngRVoyVGJPygJFeABmsCAgBPPTMM880z3OelSokheXkh+vAP7/AZt8NJT8MVux+Dp/Ia8ZZJRaD7MJlQ+b6bmboEHCOpeMgvxBwXnusiIYPQwO29gWsMFl+mOxs1qKzWIHGABZpPCwz7bLLLvkllP8uq6ps8oma5tXOv0uyHhN4UeUPYJUJj5PCGTBZ5x122IFZ3eLZcF/crcH3Tw8YMABTLgSBAsNaIQKBzwzJxzU7ppHr0FZXBD286WA75A4YhV/UBj4/l8UNxCPIwFVYMKJcMAr2Cleg7e4GG2wAnbNmgAs3vygLl7ydod0MUsE0BEFCgfR02NmWlzkCz/CsFfRk+R5DFjRXTh0xTTdaF7FkmEwhVx5NhcPFTu6SnVU0bkGfrMyhvQGjm1gRMrAHIHyYdRooTEv2p0cBNpgxG9JMjBZISElJE1PE2muvjVEhyljrZa3411p+Y9MjEdv6BV2MsOh78XlRkfCwb5tUhPhIspyt+TXVqpsZGl8QE1j79evHOPWD/MCED96wycIoHJ8M6DnpmVEEZ5xAcQXFJplUlXj50T57YyUqUgGlaSUCke9b0mmJlRROHLIqYdweNGhQ8eo5Q6MHsgKLjs5RRZhCIWmmUOYHmKt4aY1K2XiGhmzskD/cKVGdXRdEkWXWQxjM+RgcBInAwnDFEO37nVAHEbHxek+q3RzXBRXRbdjTfPnQoKEoaDLpZ08hYM1Zj2zNMoXe0iNVcLYU6eveocHj7ERSEIj2k9gjqK3J+yi1PnEDC2IE9Mxf2BQXdEMsFJlR/aFnctFGhyX/DcCRFSRhGhbpzbHc0sPNzFlIQg1REOvAP7/aTb2LOoURG0kO+Q8DIITNS8IrihULawqPpi+Y6PfZZ5+kh0TdFfOOrruE6mbcaaedLrzwQkxcLPnzvkHYGJawjWHXMW9KZE3eUjYZurRUU2PxQTn66KPh/kg7t0Jswo3cqWoqX4nLhkA4yWO2ZDoNa4haiM5gMSzbc5KJn+cYJksN48RgW2CYIZkZLA0mYXgHuyOXqHPG0LAGX2dAU28NYf//tu7UevcyEl5BX0QnZirEx5gBWbBANFHoCm2SXEUyogahgGKaQCqnV1hISBqZk4+mv7GEM4dSPToGZk2maUYMRhjWqnl6snU0HAqhPkXqX7BumP3hIfoC2s5yYi9YVFay+vDPKq158Sx/YERhA2X0CBgCowKjLlW0ihL3eInLiTun4Dbopx31mLHzEvAyY2lE5o6aBmFj3GLlOKkGRSmzLjERoZoj8fMBPUYuEi0DmZccERYZlyHGx+IQ9Fk1i45FyipQ8eVHAAGa03mpJ5vmEaz9NCpikAVZt2KuI4xYhu02aydLajPRvDGtcQtLLe47noZpgSVRLnnfeNMIYLNZf/310ZeSp1F5rgYGmsvQDayoihICjUIAsxWeByzSY13A341FBKwU9alxqVWicN+OybJFwY/qpBbVGZEIiKDND2EdvsSgDeDIxHW3jrUwlBu3z2WVgy0EKb+4YpBVjuJLggAKFZYYxi8KDN/fczMYZxjYrgEENdeeflwAAANlSURBVKwynCBb63DGDGOOihzqGe6x5OxP809iUNsaK5IldiA3iTcbGTF0sxFW+V2BwB577IHGhgkEpzwMYibL03LWaLCGwUbwBO5p4fd2ugKXJjQSsxwcn7UtM3wg7mmc8hHGKFx1BOhQuBlpL2oIA40ziHbeeWdWKqNbRS4xdNmrwpppaL7mHDd8JigBKZNVWpbDGMUcpYAaXaTY3qdp/Dp07+ukEoRA5RDgY7S42ePEwGI2ixTI8vywu/LD4wHTK5wdrqJVroHlqTArREXomQrb59vLU3PVpPcI4MaEsw7uNSi1/NwMtsIKK7BsXF/5rJBitSYvzozo6GEhsL4xND6PrMiwRob7GMsrYZqmhqVDNxVeFS4EhIAQEAKlRgBvCfxncabp378/fotRXfEwxSERFwfikfnYJYgIHqVp3qUYunnYqmQhIASEgBCoAAIYt3FZYNU51WuB/QgsXeHeyDalBm70KIKLGLoISkojBISAEBACQqDVCBTd/tTqeul5QkAICAEhIAS6GwExdHf3v1ovBISAEBACZUVADF3WnlG9hIAQEAJCoLsREEN3d/+r9UJACAgBIVBWBMTQZe0Z1UsICAEhIAS6GwExdHf3v1ovBISAEBACZUVADF3WnlG9hIAQEAJCoLsREEN3d/+r9UJACAgBIVBWBMTQZe0Z1UsICAEhIAS6GwExdHf3v1ovBISAEBACZUVADF3WnlG9hIAQEAJCoLsREEN3d/+r9UJACAgBIVBWBMTQZe0Z1UsICAEhIAS6GwExdHf3v1ovBISAEBACZUVADF3WnlG9hIAQEAJCoLsREEN3d/+r9UJACAgBIVBWBMTQZe0Z1UsICAEhIAS6GwExdHf3v1ovBISAEBACZUVADF3WnlG9hIAQEAJCoLsREEN3d/+r9UJACAgBIVBWBMTQZe0Z1UsICAEhIAS6GwExdHf3v1ovBISAEBACZUVADF3WnlG9hIAQEAJCoLsREEN3d/+r9UJACAgBIVBWBMTQZe0Z1UsICAEhIAS6GwExdHf3v1ovBISAEBACZUVADF3WnlG9hIAQEAJCoLsREEN3d/+r9UJACAgBIVBWBMTQZe0Z1UsICAEhIAS6GwExdHf3v1ovBISAEBACZUVADF3WnlG9hIAQEAJCoLsR+F8iybcLw+VhZAAAAABJRU5ErkJggg==)\n",
        "\n",
        "In a hierarchical model, parameters are viewed as a sample from a population distribution of parameters. Thus, we view them as being neither entirely different nor exactly the same. This is ***partial pooling***:\n",
        "\n",
        "![partial_pooled_model.png](data:image/png;base64,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)\n",
        "\n",
        "We can use PyMC to easily specify multilevel models, and fit them using Markov chain Monte Carlo."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pkRQRMv_YHCs"
      },
      "source": [
        "## Partial pooling model\n",
        "\n",
        "The simplest partial pooling model for estimating home run rate is one where we allow individual rates to vary, but constrained by a *population model*. A partial pooling model is essentially a weighted average (based on sample size) of a pooled (population mean) and unpooled model.\n",
        "\n",
        "You don't have to be Bayesian to do this!\n",
        "\n",
        "$$\\hat{p} \\approx \\frac{(n_j/\\sigma_p^2)\\bar{p}_j + (1/\\sigma^2)\\bar{p}}{(n_j/\\sigma_p^2) + (1/\\sigma^2)}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "HuCe6Yk6Ti42",
        "outputId": "3ab7335c-b438-4ba5-9e14-bfcae65eacee"
      },
      "outputs": [],
      "source": [
        "rates = hr / pa\n",
        "var_rates = rates * (1 - rates) \n",
        "\n",
        "pop_rate = fitting_subset.hr.sum() / fitting_subset.pa.sum()\n",
        "pop_var = pop_rate * (1 - pop_rate) \n",
        "\n",
        "partial_pooled_rates = (rates * (1/var_rates) + pop_rate * (1/pop_var)) / (1/var_rates + 1/pop_var)\n",
        "\n",
        "sns.kdeplot(partial_pooled_rates, label='partial pooling')\n",
        "sns.kdeplot(rates, label='unpooled', clip=(0, 1))\n",
        "plt.legend()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JULcPkvZXKEU"
      },
      "source": [
        "To build a Bayesian hierarchical model, we need to move from simply specifying a prior on the latent home run rates to constructing a **population model** for them. For example, we can hypothesize that MLB home run rates are distributed as a beta distribution, with unknown shape parameters $\\alpha$ and $\\beta$.\n",
        "\n",
        "$$ p_i \\sim \\text{Beta}(\\alpha, \\beta) $$\n",
        "$$ \\alpha, \\beta \\sim p() $$\n",
        "\n",
        "The model will then seek to estimate $\\alpha$ and $\\beta$ from the data. These estimates will be more strongly influenced by batters with more plate appearances, and will shrink hitters with fewer plate appearances toward the population mean.\n",
        "\n",
        "> How do we choose priors for $\\alpha$ and $\\beta$?\n",
        "\n",
        "A natural choice is the **gamma distribution**. The gamma distribution is a continuous distribution that is defined for all positive values of $x$, and is parameterized by shape $\\alpha$ and scale $\\beta$.\n",
        "\n",
        "$$ f(x \\mid \\alpha, \\beta) =\n",
        "           \\frac{\\beta^{\\alpha}x^{\\alpha-1}e^{-\\beta x}}{\\Gamma(\\alpha)}\n",
        "$$\n",
        "\n",
        "![](https://www.pymc.io/projects/docs/en/stable/_downloads/b66a9582ce2b1beb39697efd31ee89da/pymc-Gamma-1.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rather than assuming each observation is independent, we will hypothesize a latent home run rate for each player (ignoring temporal dynamics for now). Therefore we will need to have a unique index for each batter that can be used to look up individual latent home run rates, and use it for each year that a given player appears in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batter_idx, batters = fitting_subset.batter_name.factorize()\n",
        "\n",
        "coords = {'batter': batters}\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can start by specifying our home run rate population model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with pm.Model(coords=coords) as beta_partial_pooling_model:\n",
        "\n",
        "    alpha_params = pm.find_constrained_prior(\n",
        "        pm.Gamma,\n",
        "        lower=0.05,\n",
        "        upper=2,\n",
        "        init_guess={\"alpha\": 2, \"beta\": 1},\n",
        "    )\n",
        "    alpha = pm.Gamma(\"alpha\", **alpha_params)\n",
        "\n",
        "    beta_params = pm.find_constrained_prior(\n",
        "        pm.Gamma,\n",
        "        lower=8,\n",
        "        upper=12,\n",
        "        init_guess={\"alpha\": 10, \"beta\": 1},\n",
        "    )\n",
        "    beta = pm.Gamma(\"beta\", **beta_params)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the rest of the model is the same as the non-hierarchical version. Note that now, however, we have latent paramters for the beta distribution, and not just constants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with beta_partial_pooling_model:\n",
        "\n",
        "    p = pm.Beta(\"p\", alpha, beta, dims='batter')\n",
        "\n",
        "    y = pm.Binomial('y', n=pa, p=p[batter_idx], observed=hr)\n",
        "\n",
        "pm.model_to_graphviz(beta_partial_pooling_model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The prior predictive sample checks out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with beta_partial_pooling_model:\n",
        "    beta_partial_pooling_trace = pm.sample_prior_predictive(1000)\n",
        "\n",
        "az.plot_ppc(beta_partial_pooling_trace, group='prior', kind='cumulative');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with beta_partial_pooling_model:\n",
        "    beta_partial_pooling_trace.extend(pm.sample(chains=4, cores=4, random_seed=RANDOM_SEED))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with beta_partial_pooling_model:\n",
        "    pm.sample_posterior_predictive(beta_partial_pooling_trace, extend_inferencedata=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "az.plot_ppc(beta_partial_pooling_trace, group='posterior', kind='cumulative', ax=axes[0]);\n",
        "az.plot_ppc(beta_partial_pooling_trace, group='posterior', ax=axes[1]);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's do a quick comparison between the non-hierarchical and hierarchical models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(20, 6), sharex=True, sharey=True)\n",
        "\n",
        "N_pa = fitting_subset.pa.values\n",
        "for ax, trace, level, idx in zip(\n",
        "    axes,\n",
        "    (uninformed_prior_trace, beta_partial_pooling_trace),\n",
        "    (\"no pooling\", \"partial pooling\"),\n",
        "    (slice(0, len(N_pa)), batter_idx)\n",
        "):\n",
        "\n",
        "    alpha = beta_partial_pooling_trace.posterior.alpha.mean()\n",
        "    beta = beta_partial_pooling_trace.posterior.beta.mean()\n",
        "    ax.hlines(\n",
        "        alpha / (alpha + beta),\n",
        "        0.,\n",
        "        N_pa.max() + 1,\n",
        "        # alpha=0.4,\n",
        "        ls=\"--\",\n",
        "        label=\"Est. population mean\",\n",
        "    )\n",
        "\n",
        "    # plot hdi\n",
        "    hdi = az.hdi(trace).p[idx]\n",
        "    ax.vlines(N_pa, hdi.sel(hdi=\"lower\"), hdi.sel(hdi=\"higher\"), color=\"orange\", alpha=0.5)\n",
        "\n",
        "    # plot means\n",
        "    ax.plot(\n",
        "        N_pa,\n",
        "        trace.posterior.mean(dim=(\"chain\", \"draw\")).p[idx], \n",
        "        'b.', alpha=0.5\n",
        "    )\n",
        "\n",
        "    ax.set(\n",
        "        title=f\"{level.title()} Estimates\",\n",
        "        xlabel=\"Plate appearances\",\n",
        "        # xscale=\"log\",\n",
        "        ylabel=\"HR Rate\",\n",
        "        ylim=(0., 0.4)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swD9fceYYHCv"
      },
      "source": [
        "Notice the difference between the unpooled and partially-pooled estimates, particularly at smaller sample sizes: As expected, the former are both more extreme and more imprecise. Indeed, in the partially-pooled model, estimates in small-sample-size counties are informed by the population parameters -- hence more precise estimates. Moreover, the smaller the sample size, the more regression towards the overall mean (the dashed gray line) -- hence less extreme estimates. In other words, the model is skeptical of extreme deviations from the population mean in counties where data is sparse. \n",
        "\n",
        "Notice also how the right side of the plot has not changed much (since they are driving the mean, rather than being shrunk towards it)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tmH8ptOJYHCv"
      },
      "source": [
        "## Group covariate model\n",
        "\n",
        "Our first partial pooling model is a simple hierarchical random effect. This means that the parameters in our model (hr rates) are functions of other parameters (the population mean and variance). So we are modeling rates as a distribtion, from which the home-run-hitting talents of each player are drawn. \n",
        "\n",
        "But its possible that we can do better than simply specifying a single distribtion to accound for all of the variability in the population. What if the population is *heterogeneous*, and we have additional data that can be used to describe this heterogeneity. We can use this data to create a hierarchical relationship that is more than just a univariate distribtion, but an entire regression model.\n",
        "\n",
        "For example, knowing which primary position a batter plays may provide information on their home run rate, perhaps related to the characteristics of their home park. Or the primary position of the batter may be informative--first basemen tend to hit more home runs than second basemen, for example.\n",
        "\n",
        "We can expand our model of home run rates with covariates like this:\n",
        "\n",
        "$$\\text{logit}(p_i) \\sim N(\\theta_i, \\sigma^2)$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\\theta_i = \\mu_{j[i]} + \\epsilon_{i}$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\\mu_j \\sim N(m_{\\mu}, s_{\\mu}), \\epsilon_i \\sim N(0, \\sigma)$$\n",
        "\n",
        "Here, $j[i]$ signifies the mean $\\mu_j$ that corresponds to the group (position) to which batter $i$ belongs. While one could fit separate models for each group, this model fits a joint population mean $\\mu$ but allows each group to vary from it in some way.\n",
        "\n",
        "There is also a random effect $\\epsilon_i$ corresponding to each individual batter, conditional on this mean.\n",
        "\n",
        "This model is implemented in PyMC as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lnjb1WR1mxye"
      },
      "outputs": [],
      "source": [
        "position_idx, positions = fitting_subset.position.factorize()\n",
        "\n",
        "coords = {'batter': batters, 'position': positions}\n",
        "\n",
        "with pm.Model(coords=coords) as position_means_model:\n",
        "\n",
        "    # Group means\n",
        "    m_mu = pm.Normal('m_mu', mu=-2, sigma=1)\n",
        "    s_mu = pm.HalfNormal('s_mu', 1)\n",
        "    mu = pm.Normal('mu', mu=m_mu, sigma=s_mu, dims='position')\n",
        "\n",
        "    # Individual random effects\n",
        "    sigma = pm.HalfNormal('sigma', 1)\n",
        "    epsilon = pm.Normal('epsilon', mu=0, sigma=sigma, dims='batter')\n",
        "    \n",
        "    p = pm.Deterministic('p', pm.math.invlogit(mu[position_idx] + epsilon[batter_idx]))\n",
        "    \n",
        "    y = pm.Binomial('y', n=pa, p=p, observed=hr)\n",
        "    \n",
        "pm.model_to_graphviz(position_means_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2HUIP-gYHCw"
      },
      "outputs": [],
      "source": [
        "with position_means_model:\n",
        "    position_means_trace = pm.sample(1000, tune=2000, chains=4, cores=4, random_seed=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "pc7IpoxknGAW",
        "outputId": "5fc7f41d-098c-4f42-f3cb-4cd9c2fa54b4"
      },
      "outputs": [],
      "source": [
        "position_means = position_means_trace.posterior[\"mu\"].mean((\"chain\", \"draw\"))\n",
        "sorted_positions = position_means_trace.posterior[\"position\"].sortby(position_means)\n",
        "\n",
        "az.plot_forest(\n",
        "    position_means_trace, \n",
        "    var_names=[\"m_mu\", \"mu\"], \n",
        "    r_hat=True, \n",
        "    combined=True, \n",
        "    figsize=(8, 10), \n",
        "    textsize=10,\n",
        "    coords={\"position\": sorted_positions},\n",
        "    transform=lambda x: 1 / (1 + np.exp(-x)),\n",
        ");"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "upJSbqvIAsmw"
      },
      "source": [
        "## Model Comparison via Predictive Information Criteria\n",
        "\n",
        "So far we have been evaluating how models peform in an absolute sense, in terms of their fit to the data. Often we want to know performance relative to another model, or a set of other models. A principled way of doing this is using a measure of **predictive accuracy**.\n",
        "\n",
        "Measures of predictive accuracy are called information criteria, and are comprised of the log-predictive density of the data given a point estimate of the fitted model multiplied by −2 (*i.e.* the deviance):\n",
        "\n",
        "$$−2 \\log[p(y | \\hat{\\theta})]$$\n",
        "\n",
        "Clearly, the expected accuracy of a fitted model’s predictions of *future data* will generally be lower than the accuracy of the model’s predictions for *observed data*, even though the parameters in the model happen to be sampled from the specified prior distribution.\n",
        "\n",
        "Why are we interested in prediction accuracy?\n",
        "\n",
        "1. to quantify the performance of a model\n",
        "2. to perform model selection \n",
        "\n",
        "By model selection, we may not necessarily want to choose one model over another, but we might want to put different models on the same scale. The advantage if information-theoretic measures is that candidate models do not need to be nested; even models with completely different parameterizations can be used to predict the same measurements.\n",
        "\n",
        "Note that when candidate models have the same number of parameters, one can compare their best-fit log predictive densities directly, but when model dimensions differ, one has to make an adjustment for the tendency of a larger model to fit data better.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Akaike's Information Criterion (AIC)\n",
        "\n",
        "One approach to model selection is to use an information-theoretic criterion to identify the most appropriate model. Akaike (1973) found a formal relationship between Kullback-Leibler information (a dominant paradigm in information and coding theory) and likelihood theory. Akaike's Information Criterion (AIC) is an estimator of expected relative K-L information based on the maximized log-likelihood function, corrected for asymptotic bias.\n",
        "\n",
        "$$\\text{AIC} = −2 \\log(L(\\theta|data)) + 2K$$\n",
        "\n",
        "AIC balances the fit of the model (in terms of the likelihood) with the number of parameters required to achieve that fit. We can easily calculate AIC from the residual sums of squares as:\n",
        "\n",
        "$$\\text{AIC} = n \\log(\\text{RSS}/n) + 2k$$\n",
        "\n",
        "where $k$ is the number of parameters in the model. Notice that as the number of parameters increase, the residual sum of squares goes down, but the second term (a penalty) increases.\n",
        "\n",
        "A limitation of AIC for Bayesian models is that it cannot be applied to hierarchical models (or any models with random effects), as counting the number of parameters in such models is problematic. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Expected Log Predictive Density (ELPD)\n",
        "\n",
        "The expected log pointwise predictive density (ELPD) is a Bayesian information criterion that can be used to compare non-nested models. It is based on the log pointwise predictive density (LPD), which is the log predictive density of each data point given the rest of the data and the model. \n",
        "\n",
        "The LPD is a sum of the log predictive density of each data point, and is therefore sensitive to the number of data points. The ELPD is the expected value of the LPD, and is therefore insensitive to the number of data points. \n",
        "\n",
        "The ELPD is computed by taking the log predictive density of each data point, and weighting it by the posterior probability of the model given the data. The ELPD is therefore a better measure of predictive accuracy than the LPD.\n",
        "\n",
        "$$\n",
        "\\operatorname{ELPD}=\\sum_{i=1}^N \\int \\mathrm{d} y^{new}_i f^*\\left(y^{new}_i\\right) \\ln f\\left(y^{new}_i \\mid y^{obs}\\right)\n",
        "$$\n",
        "\n",
        "Unfortunately, since $f^*\\left(y^{new}_i\\right)$ (the **true** model) is not known we cannot calculate ELPD directly (just as we typically cannot calculate KL distance directly). We are therefore forced to derive approximations of ELPD based on data sampled from the model."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Widely-applicable Information Criterion (WAIC)\n",
        "\n",
        "WAIC (Watanabe 2010) is a fully Bayesian criterion for estimating out-of-sample expectation, using the log pointwise posterior predictive density (LPPD) and correcting for the effective number of parameters to adjust for overfitting.\n",
        "\n",
        "The computed log pointwise predictive density is:\n",
        "\n",
        "$$lppd_{comp} = \\sum_{i=1}^N \\log \\left(\\frac{1}{M} \\sum_{j=1}^M p(y_i | \\theta^{(j)}) \\right)$$\n",
        "\n",
        "The complexity adjustment here is as follows:\n",
        "\n",
        "$$p_{WAIC} = 2\\sum_{i=1}^N \\left[ \\log \\left(\\frac{1}{M} \\sum_{j=1}^M p(y_i | \\theta^{(j)})\\right)  - \\frac{1}{M} \\sum_{j=1}^M \\log p(y_i | \\theta^{(j)})  \\right]$$\n",
        "\n",
        "so WAIC is then:\n",
        "\n",
        "$$\\text{WAIC} = -2(lppd) + 2p_{WAIC}$$\n",
        "\n",
        "The adjustment is an approximation to the number of unconstrained parameters in the model (0=fully constrained, 1=no constraints). In this sense, WAIC treats the effective number of paramters as a random variable.\n",
        "\n",
        "WAIC averages over the posterior distribution, and therefore is more reliable for a wider range of models."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ArviZ includes functions for calculating predictive information criteria, with the caveat that you generally need to calculate the model log likelihood manually, as it is not saved during MCMC sampling by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with uninformative_prior_model:\n",
        "    pm.compute_log_likelihood(uninformed_prior_trace)\n",
        "    \n",
        "az.waic(uninformed_prior_trace, uninformative_prior_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with informative_prior_model:\n",
        "    pm.compute_log_likelihood(informed_prior_trace)\n",
        "\n",
        "az.waic(informed_prior_trace, informative_prior_model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Leave-one-out Cross-validation (LOO)\n",
        "\n",
        "LOO cross-validation is an estimate of the out-of-sample predictive fit. In cross-validation, the data are repeatedly partitioned into training and holdout sets, iteratively fitting the model with the former and evaluating the fit with the holdout data. \n",
        "\n",
        "The estimate of out-of-sample predictive fit from applying LOO cross-validation to a Bayesian model is:\n",
        "\n",
        "$$lppd_{loo} = \\sum_{i=1}^N \\log p_{post(-i)}(y_i) =  \\sum_{i=1}^N \\log \\left(\\frac{1}{S} \\sum_{s=1}^S p(y_i| \\theta^{(is)})\\right)$$\n",
        "\n",
        "so, each prediction is conditioned on $N-1$ data points, which induces an underestimation of the predictive fit for smaller $N$. The resulting estimate of effective samples size is:\n",
        "\n",
        "$$p_{loo} = lppd - lppd_{loo}$$\n",
        "\n",
        "As mentioned, using cross-validation for a Bayesian model, fitting $N$ copies of the model under different subsets of the data is computationally expensive. However, Vehtari et al. (2016) introduced an efficient computation of LOO from MCMC samples, which are corrected using Pareto-smoothed importance sampling (PSIS) to provide an estimate of point-wise out-of-sample prediction accuracy.\n",
        "\n",
        "This involves estimating the importance sampling LOO predictive distribution\n",
        "\n",
        "$$p(\\tilde{y}_i | y_{-i}) \\approx \\frac{\\sum_{s=1}^S w_i(\\theta^{(s)}) p(\\tilde{y}_i|\\theta^{(s)})}{\\sum_{s=1}^S w_i(\\theta^{(s)})}$$\n",
        "\n",
        "where the importance weights are:\n",
        "\n",
        "$$w_i(\\theta^{(s)}) = \\frac{1}{p(y_i | \\theta^{(s)})} \\propto \\frac{p(\\theta^{(s)}|y_{-i})}{p(\\theta^{(s)}|y)}$$\n",
        "\n",
        "The predictive distribution evaluated at the held-out point is then:\n",
        "\n",
        "$$p(y_i | y_{-i}) \\approx \\frac{1}{\\frac{1}{S} \\sum_{s=1}^S \\frac{1}{p(y_i | \\theta^{(s)})}}$$\n",
        "\n",
        "However, the posterior is likely to have a smaller variance and thinner tails than the LOO posteriors, so this approximation induces instability due to the fact that the importance ratios can have high or infinite variance.\n",
        "To deal with this instability, a generalized Pareto distribution fit to the upper tail of the distribution of the importance ratios can be used to construct a test for a finite importance ratio variance. If the test suggests the variance is infinite then importance sampling is halted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "az.loo(uninformed_prior_trace, uninformative_prior_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "az.loo(informed_prior_trace, informative_prior_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with beta_partial_pooling_model:\n",
        "    pm.compute_log_likelihood(beta_partial_pooling_trace)\n",
        "\n",
        "with position_means_model:\n",
        "    pm.compute_log_likelihood(position_means_trace)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we do a simple model comparison, we can see that both hierarchical model formulations easily outperform the pooled and unpooled models. Group means appear to slightly improve model performance, based on apprixmated out-of-sample prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "pMfG-TMGYHCw",
        "outputId": "b59fee44-a238-4eb4-d2e1-ea8c975e9375"
      },
      "outputs": [],
      "source": [
        "model_compare = az.compare(\n",
        "    {\n",
        "        \"Uninformative prior\": uninformed_prior_trace,\n",
        "        \"Informative prior\": informed_prior_trace,\n",
        "        \"Beta partial pooling\": beta_partial_pooling_trace,\n",
        "        \"Positional means\": position_means_trace\n",
        "    }\n",
        ")\n",
        "az.plot_compare(model_compare, figsize=(12, 4), insample_dev=False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "uJPtodfMCzAP",
        "outputId": "53bfb242-3e35-4dff-f0d8-4056c92ed821"
      },
      "outputs": [],
      "source": [
        "model_compare.round(2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OQG2lwNBYHCy"
      },
      "source": [
        "## Individual covariate model\n",
        "\n",
        "We have expanded our multilevel model to include a group-level factor--position--where every member of the group share an expected value. However, its possible to include covariates at **multiple levels**, provided that they exist. \n",
        "\n",
        "Let's go ahead and add an individual covariate, just one of many possible batter attributes that may influence home run rate: average exit velocity.\n",
        "\n",
        "This might look like this:\n",
        "\n",
        "$$\\text{logit}(p_i) \\sim N(\\theta_i + \\beta x_i, \\sigma^2)$$\n",
        "\n",
        "where again the group mean $\\theta$ is:\n",
        "\n",
        "$$\\theta_i = \\mu_{j[i]} + \\epsilon_{i}$$\n",
        "\n",
        "and $x_i$ is the average exit velocity for batter $i$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKcpSNruYHCy"
      },
      "outputs": [],
      "source": [
        "ev = ((fitting_subset.avg_ev - fitting_subset.avg_ev.mean()) / fitting_subset.avg_ev.std()).values\n",
        "\n",
        "with pm.Model(coords=coords) as individual_covariate_model:\n",
        "\n",
        "    m_mu = pm.Normal('m_mu', mu=-2, sigma=1)\n",
        "    s_mu = pm.HalfNormal('s_mu', 1)\n",
        "    mu = pm.Normal('mu', mu=m_mu, sigma=s_mu, dims='position')\n",
        "\n",
        "    sigma = pm.HalfNormal('sigma', 1)\n",
        "    epsilon = pm.Normal('epsilon', mu=0, sigma=sigma, dims='batter')\n",
        "\n",
        "    beta = pm.Normal('beta', mu=0, sigma=1)\n",
        "\n",
        "    logit_p = pm.Deterministic('logit_p',\n",
        "        mu[position_idx] + epsilon[batter_idx] + beta * ev,\n",
        "    )\n",
        "    \n",
        "    p = pm.Deterministic('p', pm.math.invlogit(logit_p))\n",
        "    \n",
        "    y = pm.Binomial('y', n=pa, p=p, observed=hr)\n",
        "\n",
        "pm.model_to_graphviz(individual_covariate_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36-K6-fBOQRX"
      },
      "outputs": [],
      "source": [
        "with individual_covariate_model:\n",
        "    individual_covariate_trace = pm.sample(1000, tune=2000, chains=4, cores=4, random_seed=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uaX9QIiF9FH"
      },
      "source": [
        "As you would expect, the coefficient for average EV is positive, and it soaks up some of the variation that was previously attributed to positional means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "VI55uG0HqVbf",
        "outputId": "c0db7c8b-9327-4178-fc57-8b5ac91f5017"
      },
      "outputs": [],
      "source": [
        "az.plot_trace(individual_covariate_trace, var_names=['beta']);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above are **trace plots** of the exit velicity coefficient. It shows a KDE for each of the sampled chains on the left pane and a time series of the samples on the right. \n",
        "\n",
        "Notice that not all of the distributions are smooth, and there is a series of ominous black lines along the floor of the plots. Moreover, if we inspect the output from `pm.sample` there are a couple of warnings. In particular, we are alerted to the presence of **divergent samples** in the trace.\n",
        "\n",
        "### Bad Energy?\n",
        "\n",
        "Another useful diagostic is to compare the overall distribution of \n",
        "energy levels with the *change* of energy between successive samples. Ideally, they should be very similar.\n",
        "\n",
        "If the distribution of energy transitions is narrow relative to the marginal energy distribution, this is a sign of inefficient sampling, as many transitions are required to completely explore the posterior. On the other hand, if the energy transition distribution is similar to that of the marginal energy, this is evidence of efficient sampling, resulting in near-independent samples from the posterior.\n",
        "\n",
        "These quantities can be readily plotted, using the `energy` values in the model trace:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "energy = individual_covariate_trace.sample_stats['energy'].values.ravel()\n",
        "energy_diff = np.diff(energy)\n",
        "sns.kdeplot(energy - energy.mean(), label='energy')\n",
        "sns.kdeplot(energy_diff, label='energy diff')\n",
        "plt.legend();"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is automated via the `plot_energy` function in ArviZ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "az.plot_energy(individual_covariate_trace);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bayesian Fraction of Missing Information\n",
        "\n",
        "The Bayesian fraction of missing information (BFMI) is a measure of how hard it is to\n",
        "sample level sets of the posterior at each iteration. Specifically, it quantifies **how well momentum resampling matches the marginal energy distribution**. \n",
        "\n",
        "$$\\text{BFMI} = \\frac{\\mathbb{E}_{\\pi}[\\text{Var}_{\\pi_{E|q}}(E|q)]}{\\text{Var}_{\\pi_{E}}(E)}$$\n",
        "\n",
        "$$\\widehat{\\text{BFMI}} = \\frac{\\sum_{i=1}^N (E_n - E_{n-1})^2}{\\sum_{i=1}^N (E_n - \\bar{E})^2}$$\n",
        "\n",
        "A small value indicates that the adaptation phase of the sampler was unsuccessful, and invoking the central limit theorem may not be valid. It indicates whether the sampler is able to *efficiently* explore the posterior distribution.\n",
        "\n",
        "Though there is not an established rule of thumb for an adequate threshold, values close to one are optimal. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Diagnosing Divergent Samples\n",
        "\n",
        "Divergent samples are a warning sign that the MCMC sampler is having trouble exploring the posterior distribution. In particular, they are a sign that the sampler is having trouble negotiating the geometry of the model's posterior distribution. While MCMC algorithms are guaranteed to converge to the correct posterior distribution, and will do so quickly provided that geometric ergodicity conditions are met.\n",
        "\n",
        "Unfortunately, proving geometric ergodicity is infeasible for most models. Instead we must rely on empirical diagnostics that identify obstructions to geometric ergodicity, and hence well-behaved MCMC estimators. Hamiltonian Monte Carlo, for example, is especially powerful in this regard as its failures to be geometrically ergodic with respect to any target distribution manifest in distinct behaviors that have been developed into sensitive diagnostics. One of these behaviors is the appearance of divergences that indicate the Hamiltonian Markov chain has encountered regions of high curvature in the target distribution which it cannot adequately explore.\n",
        "\n",
        "A few randomly-occurring divergences are generally not worth worrying about. However, if the number of divergences is large, or if they are concentrated in a particular region of parameter space, then it is likely that the sampler is not exploring the posterior distribution adequately. \n",
        "\n",
        "Let's take a closer look at the location of some of the divergent samples in our model.\n",
        "\n",
        "The ArviZ `plot_parallel` generates a coordinate plot of selected variables that highlight divergent samples. This allows us to detect patterns in the divergences, if any."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "az.plot_parallel(individual_covariate_trace, var_names=[\"mu\"], norm_method=\"normal\");\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Divergences frequently pop up in hierarchical models, where the the distrbution of individual random effects changes sharply as the variance of the random effect changes. \n",
        "\n",
        "When the random effect variance is small, this implies that the individual random means are themselves close to the population mean. This results in a *funnel*-shaped relationship between the samples of group variance and any of the slopes (particularly those with a smaller sample size). \n",
        "\n",
        "In itself, this is not a problem, since this is the behavior we expect. However, if the sampler is tuned for the wider (unconstrained) part of the parameter space, it has trouble in the areas of higher curvature. The consequence of this is that the neighborhood close to the lower bound of `s_mu` is sampled poorly. The result of this will be **biased inference**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pairplot_divergence(trace, ax=None, divergence=True, color=\"C3\", divergence_color=\"C2\", chain=1):\n",
        "    mu = individual_covariate_trace.posterior.sel(chain=chain).mu[:, 0]\n",
        "    s_mu = individual_covariate_trace.posterior.sel(chain=1).s_mu\n",
        "    if not ax:\n",
        "        _, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
        "    ax.plot(mu, s_mu, \"o\", color=color, alpha=0.5)\n",
        "    if divergence:\n",
        "        divergent = trace.sample_stats[\"diverging\"][chain]\n",
        "        ax.plot(mu[divergent], s_mu[divergent], \"o\", color=divergence_color)\n",
        "    ax.set_xlabel(\"mu[0]\")\n",
        "    ax.set_ylabel(\"s_mu\")\n",
        "    ax.set_title(\"scatter plot between s_mu and mu[0]\")\n",
        "    return ax\n",
        "\n",
        "\n",
        "pairplot_divergence(individual_covariate_trace);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dealing with Divergences\n",
        "\n",
        "Having identified the divergent samples, we can now take steps to deal with them. It may be the case that the MCMC sampler simply had not completed tuning when the specified number of `tune` samples were complete. If true, we can just bump up the number of tuning samples and re-run the sampler.\n",
        "\n",
        "```python\n",
        "trace = pm.sample(tune=3000)\n",
        "```\n",
        "\n",
        "Alternatively, it is possible to force the sampler to do a better job of simulating Hamiltonian dynamics. The simulation necessarily discretizes a continuous trajectory, and the size of the discrete steps determines how close to continuous the simulation is. However, very small step sizes is computationally intensive and does not always measurably improve the quality of the simulation. We can indirectly control the step size by setting the `target_accept` argument; values closer to one will result in smoother trajectories. The default value in PyMC is 0.8, so bumping this to 0.9 or larger may help.\n",
        "\n",
        "```python\n",
        "trace = pm.sample(target_accept=0.95)\n",
        "```\n",
        "\n",
        "If this does not work, we can try to **reparameterize** the model. This is a general strategy for improving the geometry of the posterior distribution, and is especially useful for hierarchical models.\n",
        "\n",
        "The random effect is specified in a centered parameterization, where the random effect is centered on the group mean. This is a natural way to specify the model, but it can result in the behavior we are seeing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with pm.Model(coords=coords) as centered_re:\n",
        "\n",
        "    m_mu = pm.Normal('m_mu', mu=-2, sigma=1)\n",
        "    s_mu = pm.HalfNormal('s_mu', 1)\n",
        "    mu = pm.Normal('mu', mu=m_mu, sigma=s_mu, dims='position')\n",
        "\n",
        "pm.model_to_graphviz(centered_re)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a **non-centered** parameterization. By this, we mean that the random deviates are no longer explicitly modeled as being centered on `m_mu`. Instead, they are independent standard normals `z`, which are then scaled by the appropriate value of `s_mu`, before being location-transformed by the mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with pm.Model(coords=coords) as noncentered_re:\n",
        "    m_mu = pm.Normal('m_mu', mu=-2, sigma=1)\n",
        "    s_mu = pm.HalfNormal('s_mu', 1)\n",
        "    z = pm.Normal('z', mu=0, sigma=1, dims='position')\n",
        "    mu = pm.Deterministic('mu', m_mu + s_mu * z, dims='position')\n",
        "\n",
        "pm.model_to_graphviz(noncentered_re)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This model samples much better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with pm.Model(coords=coords) as noncentered_individual_covariate_model:\n",
        "\n",
        "    # Non-centered random effect!\n",
        "    m_mu = pm.Normal('m_mu', mu=-2, sigma=1)\n",
        "    s_mu = pm.HalfNormal('s_mu', 1)\n",
        "    z = pm.Normal('z', mu=0, sigma=1, dims='position')\n",
        "    mu = pm.Deterministic('mu', m_mu + s_mu * z, dims='position')\n",
        "\n",
        "    sigma = pm.HalfNormal('sigma', 1)\n",
        "    epsilon = pm.Normal('epsilon', mu=0, sigma=sigma, dims='batter')\n",
        "\n",
        "    beta = pm.Normal('beta', mu=0, sigma=1)\n",
        "\n",
        "    logit_p = pm.Deterministic('logit_p',\n",
        "        mu[position_idx] + epsilon[batter_idx] + beta * ev,\n",
        "    )\n",
        "    \n",
        "    p = pm.Deterministic('p', pm.math.invlogit(logit_p))\n",
        "    \n",
        "    y = pm.Binomial('y', n=pa, p=p, observed=hr)\n",
        "\n",
        "    noncentered_individual_covariate_trace = pm.sample(1000, tune=2000, chains=4, cores=4, target_accept=0.95, random_seed=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "_fBHT4B1YHC4",
        "outputId": "8eac6092-627a-4225-99f4-e9f1d03b070f"
      },
      "outputs": [],
      "source": [
        "ev = fitting_subset.avg_ev.values\n",
        "post = noncentered_individual_covariate_trace.posterior.assign_coords(ev=ev)\n",
        "avg_p = post[\"logit_p\"].mean(dim=(\"chain\", \"draw\")).values[np.argsort(ev)]\n",
        "avg_p_hdi = az.hdi(post, var_names=\"logit_p\")[\"logit_p\"]\n",
        "\n",
        "_, ax = plt.subplots(figsize=(10,6))\n",
        "ax.vlines(\n",
        "    ev,\n",
        "    avg_p_hdi.sel(hdi=\"lower\"),\n",
        "    avg_p_hdi.sel(hdi=\"higher\"),\n",
        "    alpha=0.2,\n",
        "    color=\"orange\",\n",
        ")\n",
        "ax.scatter(ev[np.argsort(ev)], avg_p, alpha=0.8)\n",
        "\n",
        "plt.xlim(70,105)\n",
        "plt.ylim(-6, -1)\n",
        "plt.xlabel(\"Mean exit velo\")\n",
        "plt.ylabel(\"Log(HR rate)\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LbAjRrNH_Cc"
      },
      "source": [
        "If we just look at the subset of hierarchical models we have fit so far, adding individual covariates was an important model expansion step, resulting in more precise predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "7faEcZkmw1OG",
        "outputId": "6bfff6e7-086d-40e5-9c13-c46926104983"
      },
      "outputs": [],
      "source": [
        "with noncentered_individual_covariate_model:\n",
        "    pm.compute_log_likelihood(noncentered_individual_covariate_trace)\n",
        "\n",
        "model_compare = az.compare(\n",
        "    {\n",
        "        \"Beta partial pooling\": beta_partial_pooling_trace,\n",
        "        \"Positional means\": position_means_trace,\n",
        "        \"Individual covariate\": noncentered_individual_covariate_trace\n",
        "    }\n",
        ")\n",
        "az.plot_compare(model_compare, figsize=(12, 4), insample_dev=False);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Out-of-sample prediction\n",
        "\n",
        "In many cases you want to predict on unseen (hold-out) data. PyMC includes a `pm.MutableData` container to help with this. It is a wrapper around a pytensor.shared variable and allows the values of the data to be changed later. Otherwise, `pm.MutableData` objects can be used just like any other numpy array or tensor.\n",
        "\n",
        "This distinction is significant, since internally all models in PyMC are giant symbolic expressions. When you pass raw data directly into a model, you are giving PyTensor permission to treat this data as a constant and optimize it away if doing so makes sense. If you need to change this data later you may not have any way to point at it within the larger symbolic expression. Using `pm.MutableData` offers a way to point to a specific place in the symbolic expression and change what is there.\n",
        "\n",
        "Let's update the last model to use `pm.MutableData`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with pm.Model(coords=coords) as noncentered_individual_covariate_model:\n",
        "\n",
        "    ev = pm.MutableData('ev', ((fitting_subset.avg_ev - fitting_subset.avg_ev.mean()) / fitting_subset.avg_ev.std()).values)\n",
        "    pa = pm.MutableData('pa', fitting_subset.pa.values)\n",
        "    hr = pm.MutableData('hr', fitting_subset.hr.values)\n",
        "    position_ind = pm.MutableData('position_idx', position_idx)\n",
        "    batter_ind = pm.MutableData('batter_idx', batter_idx)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The rest of the model remains unchanged, other than perhaps some data variable renaming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with noncentered_individual_covariate_model:\n",
        "\n",
        "    m_mu = pm.Normal('m_mu', mu=-2, sigma=1)\n",
        "    s_mu = pm.HalfNormal('s_mu', 1)\n",
        "    z = pm.Normal('z', mu=0, sigma=1, dims='position')\n",
        "    mu = pm.Deterministic('mu', m_mu + s_mu * z, dims='position')\n",
        "\n",
        "    sigma = pm.HalfNormal('sigma', 1)\n",
        "    epsilon = pm.Normal('epsilon', mu=0, sigma=sigma, dims='batter')\n",
        "\n",
        "    beta = pm.Normal('beta', mu=0, sigma=1)\n",
        "\n",
        "    logit_p = pm.Deterministic('logit_p',\n",
        "        mu[position_ind] + epsilon[batter_ind] + beta * ev,\n",
        "    )\n",
        "    \n",
        "    p = pm.Deterministic('p', pm.math.invlogit(logit_p))\n",
        "    \n",
        "    y = pm.Binomial('y', n=pa, p=p, observed=hr)\n",
        "\n",
        "    noncentered_individual_covariate_trace = pm.sample(1000, tune=2000, chains=4, cores=4, target_accept=0.95, random_seed=RANDOM_SEED)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's use the 2023 data to-date as the holdout set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predict_subset = hr_data[hr_data.season==2023].dropna()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because of the structure of our model, we have a bit of a bookkeeping challenge on our hands. The prediction data contains some players that were not part of the training data from the preceeding years. Since we have random effects in our model, we need to take care that the estimated random effects are associated with the same players in the prediction data.\n",
        "\n",
        "Let's start by identifying the players that we have seen before and those that are new. We can use Python's set operations to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "existing_batters = set(fitting_subset.bam_id).intersection(predict_subset.bam_id)\n",
        "len(existing_batters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_batters = set(predict_subset.bam_id).difference(set(fitting_subset.bam_id))\n",
        "new_batters"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Having these two distinct sets, we can go about extracting the associated data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "existing_batter_df = (predict_subset\n",
        "    .assign(ev=((predict_subset.avg_ev - predict_subset.avg_ev.mean()) / predict_subset.avg_ev.std()))\n",
        "    .loc[predict_subset.bam_id.isin(existing_batters), [\"batter_name\", \"position\", \"ev\", \"pa\", \"hr\"]]\n",
        "    .reset_index(drop=True)\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the players from the fitting subset, we can use the `pm.set_data` function to replace the values in each of the `pm.MutableData` objects with the new values. \n",
        "\n",
        "Getting the correct index values for the random effects is the key, which I've done using a list comprehension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "existing_batter_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with noncentered_individual_covariate_model:\n",
        "\n",
        "    pm.set_data({\n",
        "        \"ev\": existing_batter_df.ev.values,\n",
        "        \"pa\": existing_batter_df.pa.values,\n",
        "        \"hr\": existing_batter_df.hr.values,\n",
        "        \"position_idx\": [positions.get_loc(name) for name in existing_batter_df.position],\n",
        "        \"batter_idx\": [batters.get_loc(name) for name in existing_batter_df.batter_name]\n",
        "    })"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As for the previously-unseen batters, we create a similar dataset, but instead of retrieving indices from the fit model, we just create a new set of unique indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_batter_df = (predict_subset\n",
        "    .assign(ev=((predict_subset.avg_ev - predict_subset.avg_ev.mean()) / predict_subset.avg_ev.std()))\n",
        "    .loc[predict_subset.bam_id.isin(new_batters), [\"bam_id\", \"batter_name\", \"position\", \"ev\", \"pa\", \"hr\"]]\n",
        "    .reset_index(drop=True)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_position_idx = new_batter_df.position.apply(lambda x: positions.get_loc(x)).values\n",
        "new_batter_idx = new_batter_df.index.values\n",
        "new_ev = new_batter_df.ev.values\n",
        "new_pa = new_batter_df.pa.values\n",
        "new_hr = new_batter_df.hr.values"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For these new players, we add additional variables to the model. Specifically, we want to draw additional random effects for them from the fitted distribution.\n",
        "\n",
        "These, in turn, can be fed to the downstream variables, each with a variable name that is unique to the set of new players."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with noncentered_individual_covariate_model:\n",
        "\n",
        "    noncentered_individual_covariate_model.add_coord('batter_new', new_batter_df.batter_name.values)\n",
        "    epsilon_new = pm.Normal('epsilon_new', mu=0, sigma=sigma, shape=len(new_batters))\n",
        "\n",
        "    logit_p_new = pm.Deterministic('logit_p_new',\n",
        "        mu[new_position_idx] + epsilon_new[new_batter_idx] + beta * new_ev,\n",
        "    )\n",
        "    \n",
        "    p_new = pm.Deterministic('p_new', pm.math.invlogit(logit_p_new))\n",
        "    \n",
        "    y_new = pm.Binomial('y_new', n=new_pa, p=p_new, observed=new_hr, dims='batter_new')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Its worth taking a look at the model DAG, just to see if everything makes sense."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pm.model_to_graphviz(individual_covariate_model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, it's just a matter of sampling from the posterior predictive distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with noncentered_individual_covariate_model:\n",
        "    ppc = pm.sample_posterior_predictive(noncentered_individual_covariate_trace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "az.plot_ppc(ppc);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The fit looks good -- below are the posterior predictive distributions for the new players, and the number of home runs they have hit so far this season (red vertical lines)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "axes = az.plot_density(ppc, group='posterior_predictive', var_names=[\"y_new\"])\n",
        "for a,hr in zip(axes.flat, new_hr):\n",
        "    a.axvline(hr, 0, 10, color=\"red\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Workflow Steps\n",
        "\n",
        "1. Data visualization\n",
        "2. Build provisional model\n",
        "3. Prior predictive checks\n",
        "4. Fit model\n",
        "5. Assess convergence\n",
        "6. Posterior predictive checks\n",
        "7. Improve model\n",
        "8. GOTO 5"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "\n",
        "To delve into any of the topics covered in this tutorial in more detail, I recommend the following resources:\n",
        "\n",
        "- [ArviZ Project, Working with InferenceData](https://python.arviz.org/en/stable/getting_started/WorkingWithInferenceData.html)\n",
        "- [Betancourt, M., A Conceptual Introduction to Hamiltonian Monte Carlo](https://arxiv.org/abs/1701.02434)\n",
        "- [Betancourt, M., Diagnosing Biased Inference with Divergences](https://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html)\n",
        "- [Gelman, A., et. al., (2020) Bayesian Data Analysis, 3rd Edition](http://www.stat.columbia.edu/~gelman/book/)\n",
        "- [Gelman, A., et. al., Bayesian Workflow](https://arxiv.org/abs/2011.01808)\n",
        "- [Pilon, C.D., Probabilistic Programming & Bayesian Methods for Hackers](https://dataorigami.net/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/)\n",
        "- [PyMC Project, Diagnosing Biased Inference with Divergences](https://www.pymc.io/projects/examples/en/latest/diagnostics_and_criticism/Diagnosing_biased_Inference_with_Divergences.html)\n",
        "- [Vehtari, A., Comparison of MCMC effective sample size estimators](https://avehtari.github.io/rhat_ess/ess_comparison.html)\n",
        "- [Vehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing. 27(5)](https://arxiv.org/abs/1507.04544)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Raw Cell Format",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
